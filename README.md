# ğŸ¤– TÃ¼rkÃ§e Sentetik Veri Ãœretimi ve NLP: KapsamlÄ± KarÅŸÄ±laÅŸtÄ±rmalÄ± Ã‡alÄ±ÅŸma

<div align="center">

[![TÃ¼rkÃ§e](https://img.shields.io/badge/Dil-TÃ¼rkÃ§e-red.svg)](#turkish-version) [![English](https://img.shields.io/badge/Language-English-blue.svg)](#english-version)

</div>

---

<a name="turkish-version"></a>
## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon

[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE.md)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.x-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.x-yellow.svg)](https://huggingface.co/transformers/)

Bu proje, **TÃ¼rkÃ§e metinler** iÃ§in sentetik veri Ã¼retimi ve duygu analizi Ã¼zerine **9 farklÄ± derin Ã¶ÄŸrenme ve AI yaklaÅŸÄ±mÄ±nÄ±n** kapsamlÄ± karÅŸÄ±laÅŸtÄ±rmalÄ± analizini sunar. Proje kapsamÄ±nda **BERT**, **LSTM**, **BiLSTM**, **GAN**, **GPT-2**, **mT5**, **Gemini API** ve **Character-level LSTM** modelleri kullanÄ±larak hem veri Ã¼retimi hem de duygu sÄ±nÄ±flandÄ±rmasÄ± gerÃ§ekleÅŸtirilmiÅŸtir.

---

## ğŸ“‹ Ä°Ã§indekiler

- [Proje HakkÄ±nda](#-proje-hakkÄ±nda)
- [YÃ¶ntemler ve SonuÃ§lar](#-yÃ¶ntemler-ve-sonuÃ§lar)
- [KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz](#-karÅŸÄ±laÅŸtÄ±rmalÄ±-analiz)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Kurulum](#-kurulum)
- [KullanÄ±m](#-kullanÄ±m)
- [Performans Metrikleri](#-performans-metrikleri)
- [LiteratÃ¼r AraÅŸtÄ±rmasÄ±](#-literatÃ¼r-araÅŸtÄ±rmasÄ±)
- [Lisans](#-lisans)

---

## ğŸ¯ Proje HakkÄ±nda

Bu araÅŸtÄ±rma projesinin ilk adÄ±mÄ±nda, **elektrikli arabalar** konusunda TÃ¼rkÃ§e sentetik veri Ã¼retimi ve duygu analizi iÃ§in dÃ¶rt farklÄ± derin Ã¶ÄŸrenme yaklaÅŸÄ±mÄ± ve 2. adÄ±mÄ±nda ise **teknoloji haber baÅŸlÄ±klarÄ±** konusunda "BERT", "LSTM", "LLMs", "mT5" modelleri ile kapsamlÄ± bir ÅŸekilde incelemektedir:

### ğŸ”¬ AraÅŸtÄ±rma SorularÄ±
1. Hangi model TÃ¼rkÃ§e metinler iÃ§in en yÃ¼ksek duygu analizi doÄŸruluÄŸunu saÄŸlar?
2. GAN, GPT-2, BERT MLM, mT5 ve Character-level LSTM hangi senaryolarda Ã¼stÃ¼n performans gÃ¶sterir?
3. Pre-trained modeller (BERT, GPT-2, mT5) ile sÄ±fÄ±rdan eÄŸitilen modeller (LSTM, GAN) arasÄ±ndaki fark nedir?
4. Gemini AI ile geleneksel modeller arasÄ±ndaki uyuÅŸma oranÄ± nedir?
5. 100 cÃ¼mleden 3000 cÃ¼mle Ã¼retiminde hangi yÃ¶ntem en kaliteli sonuÃ§larÄ± verir?

### ğŸ“ KullanÄ±m AlanlarÄ±
- DoÄŸal Dil Ä°ÅŸleme (NLP) araÅŸtÄ±rmalarÄ±
- Sentetik veri Ã¼retimi iÃ§in benchmark Ã§alÄ±ÅŸmalarÄ±
- TÃ¼rkÃ§e duygu analizi model karÅŸÄ±laÅŸtÄ±rmalarÄ±
- EÄŸitim ve akademik projeler
- LiteratÃ¼r taramasÄ± ve state-of-the-art teknik incelemesi

---

## ğŸš€ YÃ¶ntemler ve SonuÃ§lar

### 1ï¸âƒ£ **BERT ile Duygu Analizi**
ğŸ“‚ KlasÃ¶r: `/bert-sentiment-analysis/`

**Ã–zellikler:**
- `dbmdz/bert-base-turkish-cased` modeli kullanÄ±mÄ±
- 3 sÄ±nÄ±flÄ± duygu analizi (Pozitif, Negatif, NÃ¶tr)
- Fine-tuning ile TÃ¼rkÃ§e'ye Ã¶zelleÅŸtirilmiÅŸ model
- Gemini AI ile karÅŸÄ±laÅŸtÄ±rmalÄ± deÄŸerlendirme

**Performans:**
- âœ… **Test DoÄŸruluÄŸu:** %92.6
- âœ… **Gemini UyuÅŸma:** %92.3
- âœ… **En Ä°yi SÄ±nÄ±f:** Negatif (%98)
- âš ï¸ **ZayÄ±f Nokta:** NÃ¶tr sÄ±nÄ±f (%84)

**KullanÄ±lan Teknolojiler:**
- Transformers (Hugging Face)
- PyTorch
- pandas, scikit-learn

---

### 2ï¸âƒ£ **BiLSTM + BERT Sequence Embedding**
ğŸ“‚ KlasÃ¶r: `/bilstm-bert-hybrid/`

**Ã–zellikler:**
- BERT sequence embeddings (768-boyutlu vektÃ¶rler)
- 2 katmanlÄ± Bidirectional LSTM
- Kelime sÄ±rasÄ±nÄ± koruyan hibrit mimari
- GPU optimize edilmiÅŸ eÄŸitim

**Performans:**
- âœ… **Test DoÄŸruluÄŸu:** %89-92
- âœ… **SÄ±nÄ±f BazÄ±nda F1-Score:** ~%90
- âœ… **BERT Dense Layer'dan %2-4 daha iyi**
- âœ… **Gemini ile yÃ¼ksek uyuÅŸma**

**Model Mimarisi:**
```
BERT Embedding (64 Ã— 768)
    â†“
Bidirectional LSTM (128 units)
    â†“
Bidirectional LSTM (64 units)
    â†“
Dense + Dropout
    â†“
Softmax (3 sÄ±nÄ±f)
```

---

### 3ï¸âƒ£ **LSTM Duygu Analizi**
ğŸ“‚ KlasÃ¶r: `/lstm-sentiment/`

**Ã–zellikler:**
- Saf Bidirectional LSTM mimarisi
- Hafif ve hÄ±zlÄ± model
- Early stopping ve learning rate scheduling
- Gemini ile detaylÄ± karÅŸÄ±laÅŸtÄ±rma

**Performans:**
- âœ… **Test DoÄŸruluÄŸu:** %86.8
- âœ… **Gemini UyuÅŸma:** %87.5
- âœ… **EÄŸitim SÃ¼resi:** ~10 dakika (BERT'ten hÄ±zlÄ±)
- âœ… **Model Boyutu:** 1-2M parametre (BERT: 110M)
- âœ… **Bellek KullanÄ±mÄ±:** 2-3 GB (BERT: 6-8 GB)

**Ã–ne Ã‡Ä±kanlar:**
- En hafif ve hÄ±zlÄ± model
- Kaynak kÄ±sÄ±tlÄ± ortamlar iÃ§in ideal
- Makul performans/verimlilik dengesi

---

### 4ï¸âƒ£ **GAN ile Sentetik Metin Ãœretimi**
ğŸ“‚ KlasÃ¶r: `/gan-text-generation/`

**Ã–zellikler:**
- LSTM tabanlÄ± Generator ve Discriminator
- TÃ¼rkÃ§e Vikipedi verileriyle eÄŸitim
- Cosine similarity ile benzersizlik kontrolÃ¼
- Kalite skorlama sistemi

**Veri Ãœretimi PerformansÄ±:**
- âœ… **Ãœretilen CÃ¼mle:** 1000+ Ã¶zgÃ¼n cÃ¼mle
- âœ… **Ortalama Kelime:** 7-8 kelime/cÃ¼mle
- âœ… **Kalite Skoru:** 0.688/1.0
- âœ… **Benzerlik KontrolÃ¼:** %77 Ã¶zgÃ¼nlÃ¼k

**KullanÄ±m AlanlarÄ±:**
- Veri augmentation
- EÄŸitim veri seti geniÅŸletme
- Sentetik benchmark veri setleri

---

### 5ï¸âƒ£ **Gemini ile Veri Seti OluÅŸturma ve Duygu Analizi**
ğŸ“‚ KlasÃ¶r: `/gemini-dataset-generation/`

**Ã–zellikler:**
- Google Gemini 2.5 Flash API kullanÄ±mÄ±
- Batch generation (100 cÃ¼mle/istek)
- Dual hybrid kalite skorlama (FaktÃ¶rel + Perplexity)
- Semantik benzerlik filtresi

**Ãœretim Metrikleri:**
- âœ… **Ãœretilen CÃ¼mle:** 1000 adet
- âœ… **API Ä°steÄŸi:** 42 batch
- âœ… **SÃ¼re:** 50.5 dakika
- âœ… **Kalite Skoru:** 0.688 ortalama
- âœ… **Sentiment DaÄŸÄ±lÄ±mÄ±:** %40 pozitif, %40 nÃ¶tr, %20 negatif

**Perplexity Modeli:**
- `ytu-ce-cosmos/turkish-gpt2` ile doÄŸallÄ±k kontrolÃ¼

---

## ğŸ†• Sentetik Metin Ãœretimi Modelleri (Teknoloji Haberleri)

### 6ï¸âƒ£ **BERT Masked Language Model (MLM) ile Sentetik Ãœretim**
ğŸ“‚ KlasÃ¶r: `/BERT Modeli Ä°le Sentetik Metin Ãœretimi/`

**Ã–zellikler:**
- `dbmdz/bert-base-turkish-cased` modeli
- Konservatif maskeleme stratejisi (1-3 kelime)
- Temperature sampling (1.2)
- Trigram Ã§eÅŸitlilik kontrolÃ¼ (max 8 tekrar)
- Perplexity filtreleme (eÅŸik: 50.0)

**Ãœretim PerformansÄ±:**
- âœ… **Ãœretim:** 100 â†’ 3000 cÃ¼mle
- âœ… **Tekil Oran:** â‰¥ %95
- âœ… **BERTScore F1:** â‰¥ 0.85
- âœ… **Kelime Kapsama:** â‰¥ %90
- âœ… **Perplexity:** â‰¤ 50 (doÄŸal cÃ¼mleler)

**Avantajlar:**
- YÃ¼ksek kalite ve doÄŸallÄ±k
- BERT anlambilimi ile gÃ¼Ã§lÃ¼ kontrol
- GPU hÄ±zlandÄ±rmasÄ±

---

### 7ï¸âƒ£ **Gemini API ile Sentetik Ãœretim**
ğŸ“‚ KlasÃ¶r: `/Gemini Ä°le Sentetik Metin Ãœretimi/`

**Ã–zellikler:**
- Google Gemini 2.5 Flash API
- AkÄ±llÄ± prompt mÃ¼hendisliÄŸi
- Rate limiting ve retry mekanizmasÄ±
- BERT perplexity filtreleme
- Ã‡oklu kalite kontrol katmanÄ±

**Ãœretim PerformansÄ±:**
- âœ… **Ãœretim:** 100 â†’ 3000 cÃ¼mle (~23-25 dakika)
- âœ… **Tekil Oran:** â‰¥ %90
- âœ… **BERTScore F1:** â‰¥ 0.80
- âœ… **Kelime Kapsama:** â‰¥ %85
- âœ… **API Maliyet:** Ãœcretsiz katman (15 RPM)

**Avantajlar:**
- En yÃ¼ksek anlamsal tutarlÄ±lÄ±k
- DoÄŸal TÃ¼rkÃ§e dilbilgisi
- Minimum kod karmaÅŸÄ±klÄ±ÄŸÄ±

---

### 8ï¸âƒ£ **GPT-2 TÃ¼rkÃ§e ile Sentetik Ãœretim**
ğŸ“‚ KlasÃ¶r: `/Gpt-2 Modeli Ä°le Sentetik Metin Ãœretimi/`

**Ã–zellikler:**
- `ytu-ce-cosmos/turkish-gpt2` (~124M parametre)
- Causal Language Modeling (CLM)
- Batch generation (10 cÃ¼mle/batch)
- Temperature sampling (1.0-1.5)
- KapsamlÄ± regex temizleme (15 katman)

**Ãœretim PerformansÄ±:**
- âœ… **Ãœretim:** 100 â†’ 3000 cÃ¼mle (~22-25 dakika)
- âœ… **Tekil Oran:** â‰¥ %95
- âœ… **BERTScore F1:** â‰¥ 0.75 (CLM iÃ§in)
- âœ… **Ã‡eÅŸitlilik:** Ã‡ok yÃ¼ksek
- âš ï¸ **Temizleme:** YÃ¼ksek gereksinim

**Avantajlar:**
- YÃ¼ksek Ã§eÅŸitlilik
- AkÄ±cÄ± metin Ã¼retimi
- Ãœcretsiz ve hÄ±zlÄ±

---

### 9ï¸âƒ£ **Character-level LSTM ile Sentetik Ãœretim**
ğŸ“‚ KlasÃ¶r: `/LSTM Modeli Ä°le Sentetik Metin Ãœretimi/`

**Ã–zellikler:**
- SÄ±fÄ±rdan eÄŸitilen LSTM (3.74M parametre)
- Character-level tokenization (75 karakter)
- 2 katmanlÄ± Bidirectional LSTM
- Prefix-based generation
- 50 epoch eÄŸitim (~5-10 dakika)

**Ãœretim PerformansÄ±:**
- âœ… **Ãœretim:** 100 â†’ 3000 cÃ¼mle
- âœ… **Tekil Oran:** â‰¥ %90
- âš ï¸ **BERTScore F1:** â‰¥ 0.75 (dÃ¼ÅŸÃ¼k)
- âš ï¸ **Kelime Kapsama:** â‰¥ %80
- âš ï¸ **Perplexity:** â‰¤ 70 (gevÅŸek eÅŸik)

**Avantajlar:**
- En kÃ¼Ã§Ã¼k model (~15 MB)
- HÄ±zlÄ± eÄŸitim (5-10 dk)
- DÃ¼ÅŸÃ¼k GPU memory (~500 MB)

---

### ğŸ”Ÿ **mT5 (Multilingual T5) ile Sentetik Ãœretim**
ğŸ“‚ KlasÃ¶r: `/mT5 Modeli Ä°le Sentetik Metin Ãœretimi/`

**Ã–zellikler:**
- Ä°ki model: `google/mt5-base` (580M) ve `Turkish-NLP/t5-efficient-base-turkish` (220M)
- Encoder-Decoder mimarisi
- Paraphrase, rewrite, generate gÃ¶revleri
- 15 katmanlÄ± agresif temizleme
- Dil filtreleme (Kiril, Yunanca, Ã‡ince)

**Ãœretim PerformansÄ±:**
- âœ… **Ãœretim:** 100 â†’ 3000 cÃ¼mle (~1.5-2 saat)
- âœ… **Tekil Oran:** %100 (mt5-base)
- âš ï¸ **BERTScore F1:** 0.46 (mt5-base iÃ§in dÃ¼ÅŸÃ¼k)
- âš ï¸ **Kelime Kapsama:** %50.81 (mt5-base)
- âœ… **Turkish-NLP T5:** Daha iyi performans

**Avantajlar:**
- Ã‡ok dilli destek (101 dil)
- Task flexibility
- TÃ¼rkÃ§e Ã¶zel model mevcut

---

## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz

### ğŸ† Duygu Analizi Model PerformanslarÄ± (Test Seti)

| Model | Accuracy | Precision | Recall | F1-Score | Gemini UyuÅŸma |
|-------|----------|-----------|--------|----------|---------------|
| **BERT** | **%92.6** ğŸ¥‡ | 0.926 | 0.926 | 0.926 | **%92.3** ğŸ¥‡ |
| **BiLSTM+BERT** | %89-92 ğŸ¥ˆ | ~0.90 | ~0.90 | ~0.90 | YÃ¼ksek |
| **LSTM** | %86.8 ğŸ¥‰ | 0.870 | 0.868 | 0.867 | %87.5 |

### ğŸ†• Sentetik Metin Ãœretimi Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | Tekil Oran | BERTScore F1 | Kelime Kapsama | SÃ¼re | Model Boyutu |
|-------|------------|--------------|----------------|------|--------------|
| **BERT MLM** | â‰¥%95 ğŸ¥‡ | â‰¥0.85 ğŸ¥‡ | â‰¥%90 ğŸ¥‡ | Orta | ~500 MB |
| **Gemini API** | â‰¥%90 ğŸ¥ˆ | â‰¥0.80 ğŸ¥ˆ | â‰¥%85 ğŸ¥ˆ | 23-25 dk | - (API) |
| **GPT-2** | â‰¥%95 ğŸ¥‡ | â‰¥0.75 | â‰¥%80 | 22-25 dk | ~500 MB |
| **mT5 (Turkish-NLP)** | %100 ğŸ¥‡ | YÃ¼ksek | YÃ¼ksek | 1-1.5 saat | ~900 MB |
| **mT5 (base)** | %100 ğŸ¥‡ | 0.46 âš ï¸ | %50 âš ï¸ | 1.5-2 saat | ~2.3 GB |
| **Character LSTM** | â‰¥%90 | â‰¥0.75 | â‰¥%80 | DeÄŸiÅŸken | ~15 MB ğŸ¥‡ |
| **GAN** | %77 | - | - | - | DeÄŸiÅŸken |

### âš¡ Verimlilik KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Metrik | BERT | BiLSTM+BERT | LSTM |
|--------|------|-------------|------|
| **EÄŸitim SÃ¼resi** | ~15-20 dk | ~12-15 dk | **~10 dk** âœ… |
| **Model Boyutu** | 110M param | ~60M param | **1-2M param** âœ… |
| **Bellek (GPU)** | 6-8 GB | 4-6 GB | **2-3 GB** âœ… |
| **Ã‡Ä±karÄ±m HÄ±zÄ±** | YavaÅŸ | Orta | **HÄ±zlÄ±** âœ… |

### ğŸ¯ SÄ±nÄ±f BazÄ±nda Performans

#### **Negatif SÄ±nÄ±f** (En BaÅŸarÄ±lÄ±)
- BERT: **%98** ğŸ†
- BiLSTM+BERT: ~%95
- LSTM: %93

#### **Pozitif SÄ±nÄ±f**
- BERT: **%94** ğŸ†
- BiLSTM+BERT: ~%92
- LSTM: %90

#### **NÃ¶tr SÄ±nÄ±f** âš ï¸ (TÃ¼m Modellerde ZayÄ±f)
- BERT: **%84** ğŸ†
- BiLSTM+BERT: ~%82
- LSTM: %75

### ğŸ” Temel Bulgular

#### Duygu Analizi
1. **BERT En YÃ¼ksek DoÄŸruluk**: %92.6 ile en iyi performans
2. **LSTM En Verimli**: En az kaynak, en hÄ±zlÄ± eÄŸitim
3. **BiLSTM+BERT Ä°yi Denge**: Performans-verimlilik dengesi
4. **NÃ¶tr SÄ±nÄ±f Zorlu**: TÃ¼m modellerde iyileÅŸtirme gerekli
5. **Gemini TutarlÄ±lÄ±k YÃ¼ksek**: %87-92 arasÄ± uyuÅŸma

#### Sentetik Metin Ãœretimi
1. **BERT MLM En Kaliteli**: En yÃ¼ksek BERTScore ve kelime kapsama
2. **Gemini En TutarlÄ±**: DoÄŸal TÃ¼rkÃ§e, yÃ¼ksek anlamsal tutarlÄ±lÄ±k
3. **GPT-2 En Ã‡eÅŸitli**: YÃ¼ksek Ã§eÅŸitlilik ama temizleme gerektirir
4. **Character LSTM En Hafif**: 15 MB, dÃ¼ÅŸÃ¼k kaynak kullanÄ±mÄ±
5. **mT5 Dil Sorunu**: mt5-base TÃ¼rkÃ§e'de zayÄ±f, Turkish-NLP modeli Ã¶nerilir
6. **Pre-trained > Scratch**: SÄ±fÄ±rdan eÄŸitilen modeller dÃ¼ÅŸÃ¼k kalite

### ğŸ¤” Hangi Modeli SeÃ§meli?

#### Duygu Analizi Ä°Ã§in
| Senaryo | Ã–nerilen Model | Neden? |
|---------|----------------|--------|
| **Maksimum DoÄŸruluk** | BERT | En yÃ¼ksek accuracy (%92.6) |
| **Mobil/Embedded** | LSTM | En hafif model (1-2M param) |
| **Dengeli Ã‡Ã¶zÃ¼m** | BiLSTM+BERT | Ä°yi performans + kabul edilebilir kaynak |
| **GerÃ§ek ZamanlÄ±** | LSTM | En hÄ±zlÄ± Ã§Ä±karÄ±m sÃ¼resi |

#### Sentetik Metin Ãœretimi Ä°Ã§in
| Senaryo | Ã–nerilen Model | Neden? |
|---------|----------------|--------|
| **Maksimum Kalite** | BERT MLM | En yÃ¼ksek BERTScore (â‰¥0.85) |
| **DoÄŸal TÃ¼rkÃ§e** | Gemini API | LLM ile en tutarlÄ± sonuÃ§lar |
| **Maksimum Ã‡eÅŸitlilik** | GPT-2 | YÃ¼ksek temperature sampling |
| **Minimum Kaynak** | Character LSTM | 15 MB, 500 MB GPU memory |
| **TÃ¼rkÃ§e Ã–zel** | Turkish-NLP T5 | TÃ¼rkÃ§e'ye optimize |
| **Ã‡ok Dilli** | mT5-base | 101 dil (ama TÃ¼rkÃ§e zayÄ±f) |
| **HÄ±z** | Gemini/GPT-2 | ~22-25 dakika |

---

## ğŸ“ Proje YapÄ±sÄ±

```
Sentetik-Veri-Uretimi-NLP/
â”‚
â”œâ”€â”€ README.md                                      # Ana dokÃ¼mantasyon (bu dosya)
â”œâ”€â”€ LICENSE.md                                     # Lisans bilgisi
â”‚
â”œâ”€â”€ ğŸ“Š DUYGU ANALÄ°ZÄ° MODELLERÄ°
â”‚
â”œâ”€â”€ bert-sentiment-analysis/                       # BERT Duygu Analizi
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti-5k.xlsx
â”‚   â”œâ”€â”€ bert_vs_gemini_sonuc_1k.xlsx
â”‚   â””â”€â”€ etiketsiz-test-gemini-etiketlenmis-1k.xlsx
â”‚
â”œâ”€â”€ bilstm-bert-hybrid/                            # BiLSTM + BERT Hibrit
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti.xlsx
â”‚   â”œâ”€â”€ etiketsiz-test-gemini-etiketlenmis.xlsx
â”‚   â””â”€â”€ bert_vs_gemini_sonuc.xlsx
â”‚
â”œâ”€â”€ lstm-sentiment/                                # LSTM Duygu Analizi
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti-5k.xlsx
â”‚   â”œâ”€â”€ lstm_vs_gemini_sonuc_1k.xlsx
â”‚   â””â”€â”€ etiketsiz-test-gemini-etiketlenmis-1k.xlsx
â”‚
â”œâ”€â”€ ğŸ†• SENTETÄ°K METÄ°N ÃœRETÄ°MÄ° MODELLERÄ°
â”‚
â”œâ”€â”€ BERT Modeli Ä°le Sentetik Metin Ãœretimi/        # BERT MLM
â”‚   â”œâ”€â”€ temp.py                                    # Ana script
â”‚   â”œâ”€â”€ README.md                                  # DetaylÄ± dokÃ¼mantasyon
â”‚   â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv            # 100 orijinal cÃ¼mle
â”‚   â”œâ”€â”€ sentetik_teknoloji_haberleri_3000.csv      # 3000 Ã¼retilen cÃ¼mle
â”‚   â””â”€â”€ sentetik_veri_metrikleri.png               # GÃ¶rselleÅŸtirme
â”‚
â”œâ”€â”€ Gemini Ä°le Sentetik Metin Ãœretimi/             # Gemini API
â”‚   â”œâ”€â”€ gemini_sentetik_uretim.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv
â”‚   â”œâ”€â”€ gemini_sentetik_teknoloji_haberleri_3000.csv
â”‚   â”œâ”€â”€ gemini-cÄ±ktÄ±lar.txt
â”‚   â””â”€â”€ gemini_sentetik_metrikler.png
â”‚
â”œâ”€â”€ Gpt-2 Modeli Ä°le Sentetik Metin Ãœretimi/       # GPT-2
â”‚   â”œâ”€â”€ gpt2_sentetik_uretim.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv
â”‚   â”œâ”€â”€ gpt2_sentetik_teknoloji_haberleri_3000.csv
â”‚   â””â”€â”€ gpt2-cÄ±ktÄ±lar.txt
â”‚
â”œâ”€â”€ LSTM Modeli Ä°le Sentetik Metin Ãœretimi/        # Character-level LSTM
â”‚   â”œâ”€â”€ lstm_sentetik_uretim.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv
â”‚   â”œâ”€â”€ lstm_sentetik_teknoloji_haberleri_3000.csv
â”‚   â””â”€â”€ lstm.txt
â”‚
â”œâ”€â”€ mT5 Modeli Ä°le Sentetik Metin Ãœretimi/         # mT5
â”‚   â”œâ”€â”€ t5_turkish_sentetik_uretim.py             # Turkish-NLP model
â”‚   â”œâ”€â”€ t5_sentetik_uretim.py                     # mt5-base model
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv
â”‚   â”œâ”€â”€ t5_turkish_sentetik_teknoloji_haberleri_3000.csv
â”‚   â”œâ”€â”€ t5_sentetik_teknoloji_haberleri_3000.csv
â”‚   â””â”€â”€ t5-base-duz-model.txt
â”‚
â”œâ”€â”€ gan-text-generation/                           # GAN Metin Ãœretimi (Eski)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ sentences.txt
â”‚   â”œâ”€â”€ wiki.tr.txt
â”‚   â”œâ”€â”€ uretilen_cumleler.csv
â”‚   â””â”€â”€ training_history.png
â”‚
â”œâ”€â”€ gemini-dataset-generation/                     # Gemini Veri Ãœretimi (Eski)
â”‚   â”œâ”€â”€ main10.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ main10.pdf
â”‚   â””â”€â”€ elektrikli_araba_1000_batch.xlsx
â”‚
â”œâ”€â”€ GAN Modeli Ä°le Metin Ãœretimi/                  # GAN (Ek Ã§alÄ±ÅŸma)
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ LiteratÃ¼rdeki Sentetik Veri Ãœretimi Ä°le Ä°lgili Makaleler/
    â”‚                                              # ğŸ“š LiteratÃ¼r AraÅŸtÄ±rmasÄ±
    â”œâ”€â”€ metin/                                     # Metin tabanlÄ± sentetik veri
    â”‚   â”œâ”€â”€ Genel(arxiv.org vb.)/
    â”‚   â””â”€â”€ ScienceDirect & IEEE Xplore/
    â”‚
    â”œâ”€â”€ gÃ¶rÃ¼ntÃ¼/                                   # GÃ¶rÃ¼ntÃ¼ tabanlÄ± sentetik veri
    â”‚
    â””â”€â”€ ses/                                       # Ses tabanlÄ± sentetik veri
```

---

## ğŸ› ï¸ Kurulum

### Sistem Gereksinimleri

**DonanÄ±m:**
- GPU: NVIDIA GPU (Ã¶nerilen: Tesla T4 veya Ã¼zeri)
- RAM: Minimum 8GB (Ã¶nerilen: 16GB+)
- Depolama: ~5GB (tÃ¼m modeller iÃ§in)

**YazÄ±lÄ±m:**
- Python 3.8+
- CUDA 11.x (GPU iÃ§in)
- pip veya conda

### Temel KÃ¼tÃ¼phaneler

```bash
# TÃ¼m projeler iÃ§in ortak
pip install pandas numpy openpyxl scikit-learn

# BERT projeleri iÃ§in
pip install torch transformers

# LSTM projeleri iÃ§in
pip install tensorflow

# GAN projesi iÃ§in
pip install torch sentence-transformers

# Gemini projesi iÃ§in
pip install google-generativeai
```

### Proje BazÄ±nda Kurulum

Her alt klasÃ¶rdeki README.md dosyasÄ±nda detaylÄ± kurulum talimatlarÄ± mevcuttur.

---

## ğŸš€ KullanÄ±m

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

1. **Repo'yu KlonlayÄ±n**
```bash
git clone https://github.com/kullanici-adi/synthetic-data-generation.git
cd synthetic-data-generation
```

2. **Ä°lgilendiÄŸiniz Projeye Gidin**
```bash
cd bert-sentiment-analysis/  # veya lstm-sentiment, gan-text-generation vb.
```

3. **README TalimatlarÄ±nÄ± Takip Edin**
Her klasÃ¶rdeki README.md dosyasÄ±, o projeye Ã¶zel kurulum ve Ã§alÄ±ÅŸtÄ±rma adÄ±mlarÄ±nÄ± iÃ§erir.

### Kaggle/Colab KullanÄ±mÄ±

TÃ¼m projeler **GPU destekli** ortamlarda en iyi performansÄ± gÃ¶sterir:

1. Kaggle Notebook veya Google Colab aÃ§Ä±n
2. GPU T4 x2 hÄ±zlandÄ±rÄ±cÄ±yÄ± aktif edin
3. Ä°lgili veri setlerini yÃ¼kleyin
4. `main.py` kodunu Ã§alÄ±ÅŸtÄ±rÄ±n

---

## ğŸ“ˆ Performans Metrikleri

### Accuracy (DoÄŸruluk) KarÅŸÄ±laÅŸtÄ±rmasÄ±

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 92.6% BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      91.0% BiLSTM+BERT (ortalama)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         86.8% LSTM
```

### Model Boyutu KarÅŸÄ±laÅŸtÄ±rmasÄ±

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 110M BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               60M BiLSTM+BERT
â–ˆ                                                            1.5M LSTM
```

### EÄŸitim SÃ¼resi (GPU T4 x2)

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20 dk BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      15 dk BiLSTM+BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           10 dk LSTM
```

---

## ğŸ“ Ã–ÄŸrenilen Dersler

### âœ… BaÅŸarÄ±lar

1. **BERT Ã‡ok GÃ¼Ã§lÃ¼**: Transfer learning ile TÃ¼rkÃ§e'de mÃ¼kemmel sonuÃ§lar
2. **LSTM Hala DeÄŸerli**: Kaynak kÄ±sÄ±tlÄ± senaryolar iÃ§in hÄ±zlÄ± ve etkili
3. **Hibrit YaklaÅŸÄ±m Ä°yi**: BiLSTM+BERT denge noktasÄ± saÄŸlÄ±yor
4. **GAN Ã‡alÄ±ÅŸÄ±yor**: TÃ¼rkÃ§e metin Ã¼retimi iÃ§in uygulanabilir
5. **Gemini GÃ¼venilir**: Etiketleme ve karÅŸÄ±laÅŸtÄ±rma iÃ§in tutarlÄ±

### âš ï¸ Zorluklar

1. **NÃ¶tr SÄ±nÄ±f Zor**: TÃ¼m modellerde en dÃ¼ÅŸÃ¼k performans
2. **Pozitif-NÃ¶tr KarÄ±ÅŸÄ±mÄ±**: Model ve Gemini arasÄ±nda en Ã§ok burada farklÄ±lÄ±k
3. **GAN EÄŸitimi Hassas**: Hyperparameter tuning kritik
4. **Kaynak YoÄŸunluÄŸu**: BERT modelleri bÃ¼yÃ¼k GPU bellek gerektiriyor
5. **TÃ¼rkÃ§e Veri KÄ±tlÄ±ÄŸÄ±**: Kaliteli etiketli veri bulmak zorlu

### ğŸ”® Gelecek Ä°yileÅŸtirmeler

- [ ] NÃ¶tr sÄ±nÄ±f iÃ§in Ã¶zel model eÄŸitimi
- [ ] Daha bÃ¼yÃ¼k veri setleri (10K+ cÃ¼mle)
- [ ] Transformer-XL gibi yeni mimariler
- [ ] Multi-task learning yaklaÅŸÄ±mlarÄ±
- [ ] Ensemble modelleme (BERT + LSTM)
- [ ] Fine-tuned TÃ¼rkÃ§e GPT modelleri

---

## ğŸ“š LiteratÃ¼r AraÅŸtÄ±rmasÄ±

Bu proje, sentetik veri Ã¼retimi konusunda kapsamlÄ± bir literatÃ¼r taramasÄ± iÃ§ermektedir. **LiteratÃ¼rdeki Sentetik Veri Ãœretimi Ä°le Ä°lgili Makaleler** klasÃ¶rÃ¼nde, farklÄ± veri tÃ¼rleri iÃ§in akademik Ã§alÄ±ÅŸmalar kategorize edilmiÅŸtir:

### ğŸ“ Metin TabanlÄ± Sentetik Veri
- **Genel**: Ã‡eÅŸitli kaynaklardan derlenen genel Ã§alÄ±ÅŸmalar
- **ScienceDirect & IEEE Xplore**: Akademik veritabanlarÄ±ndan seÃ§ilmiÅŸ makaleler
- GAN, LSTM, BERT ve transformer tabanlÄ± metin Ã¼retimi Ã§alÄ±ÅŸmalarÄ±
- TÃ¼rkÃ§e ve Ã§ok dilli sentetik veri Ã¼retimi yaklaÅŸÄ±mlarÄ±

### ğŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ TabanlÄ± Sentetik Veri
- Image generation iÃ§in GAN, VAE ve Diffusion modelleri
- Sentetik gÃ¶rÃ¼ntÃ¼ kalite deÄŸerlendirme metrikleri
- Computer vision uygulamalarÄ± iÃ§in veri augmentation

### ğŸ”Š Ses TabanlÄ± Sentetik Veri
- TTS (Text-to-Speech) sistemleri
- Ses senteziyle veri augmentation
- KonuÅŸma tanÄ±ma sistemleri iÃ§in sentetik veri

> **Not**: Bu klasÃ¶r, projenin teorik temelini oluÅŸturan kaynaklardan oluÅŸmaktadÄ±r ve araÅŸtÄ±rmacÄ±lar iÃ§in referans niteliÄŸindedir.

---

## ğŸ“š Referanslar ve Kaynaklar

### Modeller

- **BERT**: [dbmdz/bert-base-turkish-cased](https://huggingface.co/dbmdz/bert-base-turkish-cased)
- **Turkish GPT-2**: [ytu-ce-cosmos/turkish-gpt2](https://huggingface.co/ytu-ce-cosmos/turkish-gpt2)
- **Gemini AI**: [Google DeepMind](https://deepmind.google/technologies/gemini/)

### Veri Setleri

- **TÃ¼rkÃ§e Vikipedi**: [Turkish Sentences Dataset](https://www.kaggle.com/datasets/mahdinamidamirchi/turkish-sentences-dataset)

### KÃ¼tÃ¼phaneler

- **Transformers**: [Hugging Face](https://huggingface.co/docs/transformers/)
- **TensorFlow**: [tensorflow.org](https://www.tensorflow.org/)
- **PyTorch**: [pytorch.org](https://pytorch.org/)

---

## ğŸ¤ KatkÄ±da Bulunma

Bu proje ÅŸu anda **kapalÄ± kaynak** olup, katkÄ±lar kabul edilmemektedir. Ancak:

- ğŸ’¡ Ã–neri ve geri bildirimlerinizi paylaÅŸabilirsiniz
- â­ Projeyi beÄŸendiyseniz yÄ±ldÄ±z verebilirsiniz

---

## ğŸ“§ Ä°letiÅŸim

Proje hakkÄ±nda sorularÄ±nÄ±z iÃ§in:

- **GeliÅŸtirici**: Mustafa AtaklÄ±
- **GitHub**: [github.com/mustafaatakli](https://github.com/mustafaatakli)
- **Email**: [atakliim20@gmail.com](mailto:atakliim20@gmail.com)

---

## ğŸ“„ Lisans

```
Bu projenin tÃ¼m haklarÄ± saklÄ±dÄ±r Â© 2025 Mustafa AtaklÄ±.

Ä°zinsiz kullanÄ±mÄ±, kopyalanmasÄ± veya daÄŸÄ±tÄ±mÄ± kesinlikle yasaktÄ±r.
DetaylÄ± bilgi iÃ§in lÃ¼tfen LICENSE.md dosyasÄ±na bakÄ±nÄ±z.
```

---

## â­ YÄ±ldÄ±z Vermeyi UnutmayÄ±n!

Bu projeyi faydalÄ± bulduysanÄ±z, GitHub'da â­ vererek destek olabilirsiniz!

### ğŸ† Proje Ä°statistikleri

#### Duygu Analizi Ã‡alÄ±ÅŸmalarÄ±
- **Modeller**: 4 farklÄ± yaklaÅŸÄ±m (BERT, BiLSTM+BERT, LSTM, GAN)
- **Veri**: 10K+ etiketli cÃ¼mle
- **DoÄŸruluk**: %86.8 - %92.6 arasÄ±

#### Sentetik Metin Ãœretimi Ã‡alÄ±ÅŸmalarÄ±
- **Modeller**: 5 farklÄ± yaklaÅŸÄ±m (BERT MLM, Gemini, GPT-2, Character LSTM, mT5)
- **Veri**: 100 â†’ 3000 cÃ¼mle Ã¼retimi
- **Kalite**: BERTScore 0.46 - 0.85 arasÄ±
- **Toplam Ãœretilen**: 15,000+ sentetik cÃ¼mle

#### Genel Ä°statistikler
- **Toplam Model**: 9 farklÄ± model/yaklaÅŸÄ±m
- **Toplam Kod**: 5000+ satÄ±r Python
- **README DosyalarÄ±**: 10 adet (her model iÃ§in detaylÄ±)
- **GeliÅŸtirme SÃ¼resi**: 4+ ay
- **GPU Saati**: 150+ saat
- **LiteratÃ¼r**: 3 kategori (Metin, GÃ¶rÃ¼ntÃ¼, Ses)
- **Akademik Kaynak**: ScienceDirect & IEEE Xplore

---
---
---
# ğŸ¤– Turkish Synthetic Data Generation and Sentiment Analysis: Comparative Study
<a name="english-version"></a>
## ğŸ‡¬ğŸ‡§ English Version

[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE.md)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.x-red.svg)](https://pytorch.org/)

This project presents a comparative analysis of **four different deep learning approaches** for synthetic data generation and sentiment analysis of **Turkish texts**. Within the scope of the project, both data generation and sentiment classification were performed using **LSTM**, **BiLSTM**, **GAN**, and **BERT** models, and the results were compared with **Google Gemini AI**.

---

## ğŸ“‹ Table of Contents

- [About the Project](#-about-the-project)
- [Methods and Results](#-methods-and-results)
- [Comparative Analysis](#-comparative-analysis)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Performance Metrics](#-performance-metrics)
- [Literature Review](#-literature-review)
- [License](#-license)

---

## ğŸ¯ About the Project

In the first step of this research project, four different deep learning approaches are examined for Turkish synthetic data generation and sentiment analysis on the subject of **electric cars**, and in the second step, it is examined comprehensively with "BERT", "LSTM", "LLMs", "mT5" models on the subject of **technology news headlines**:

### ğŸ”¬ Research Questions
1. Which model provides the highest sentiment analysis accuracy for Turkish texts?
2. How original and high-quality are the results of GAN-based synthetic text generation?
3. What is the performance-resource balance of BERT and LSTM models?
4. What is the agreement rate between Gemini AI and traditional models?

### ğŸ“ Use Cases
- Natural Language Processing (NLP) research
- Benchmark studies for synthetic data generation
- Turkish sentiment analysis model comparisons
- Educational and academic projects
- Literature review and state-of-the-art technical examination

---

## ğŸš€ Methods and Results

### 1ï¸âƒ£ **BERT Sentiment Analysis**
ğŸ“‚ Folder: `/bert-sentiment-analysis/`

**Features:**
- Using `dbmdz/bert-base-turkish-cased` model
- 3-class sentiment analysis (Positive, Negative, Neutral)
- Model customized for Turkish via fine-tuning
- Comparative evaluation with Gemini AI

**Performance:**
- âœ… **Test Accuracy:** 92.6%
- âœ… **Gemini Agreement:** 92.3%
- âœ… **Best Class:** Negative (98%)
- âš ï¸ **Weak Point:** Neutral class (84%)

**Technologies Used:**
- Transformers (Hugging Face)
- PyTorch
- pandas, scikit-learn

---

### 2ï¸âƒ£ **BiLSTM + BERT Sequence Embedding**
ğŸ“‚ Folder: `/bilstm-bert-hybrid/`

**Features:**
- BERT sequence embeddings (768-dimensional vectors)
- 2-layer Bidirectional LSTM
- Hybrid architecture preserving word order
- GPU optimized training

**Performance:**
- âœ… **Test Accuracy:** 89-92%
- âœ… **Class-wise F1-Score:** ~90%
- âœ… **2-4% better than BERT Dense Layer**
- âœ… **High agreement with Gemini**

**Model Architecture:**
```
BERT Embedding (64 Ã— 768)
    â†“
Bidirectional LSTM (128 units)
    â†“
Bidirectional LSTM (64 units)
    â†“
Dense + Dropout
    â†“
Softmax (3 classes)
```

---

### 3ï¸âƒ£ **LSTM Sentiment Analysis**
ğŸ“‚ Folder: `/lstm-sentiment/`

**Features:**
- Pure Bidirectional LSTM architecture
- Lightweight and fast model
- Early stopping and learning rate scheduling
- Detailed comparison with Gemini

**Performance:**
- âœ… **Test Accuracy:** 86.8%
- âœ… **Gemini Agreement:** 87.5%
- âœ… **Training Time:** ~10 minutes (faster than BERT)
- âœ… **Model Size:** 1-2M parameters (BERT: 110M)
- âœ… **Memory Usage:** 2-3 GB (BERT: 6-8 GB)

**Highlights:**
- Lightest and fastest model
- Ideal for resource-constrained environments
- Reasonable performance/efficiency balance

---

### 4ï¸âƒ£ **GAN-based Synthetic Text Generation**
ğŸ“‚ Folder: `/gan-text-generation/`

**Features:**
- LSTM-based Generator and Discriminator
- Training with Turkish Wikipedia data
- Uniqueness check with cosine similarity
- Quality scoring system

**Data Generation Performance:**
- âœ… **Generated Sentences:** 1000+ unique sentences
- âœ… **Average Words:** 7-8 words/sentence
- âœ… **Quality Score:** 0.688/1.0
- âœ… **Similarity Check:** 77% uniqueness

**Use Cases:**
- Data augmentation
- Training dataset expansion
- Synthetic benchmark datasets

---

### 5ï¸âƒ£ **Dataset Generation and Sentiment Analysis with Gemini**
ğŸ“‚ Folder: `/gemini-dataset-generation/`

**Features:**
- Using Google Gemini 2.5 Flash API
- Batch generation (100 sentences/request)
- Dual hybrid quality scoring (Factorial + Perplexity)
- Semantic similarity filter

**Generation Metrics:**
- âœ… **Generated Sentences:** 1000
- âœ… **API Requests:** 42 batches
- âœ… **Duration:** 50.5 minutes
- âœ… **Quality Score:** 0.688 average
- âœ… **Sentiment Distribution:** 40% positive, 40% neutral, 20% negative

**Perplexity Model:**
- Naturalness check with `ytu-ce-cosmos/turkish-gpt2`

---

## ğŸ“Š Comparative Analysis

### ğŸ† Model Performance (Test Set)

| Model | Accuracy | Precision | Recall | F1-Score | Gemini Agreement |
|-------|----------|-----------|--------|----------|------------------|
| **BERT** | **92.6%** ğŸ¥‡ | 0.926 | 0.926 | 0.926 | **92.3%** ğŸ¥‡ |
| **BiLSTM+BERT** | 89-92% ğŸ¥ˆ | ~0.90 | ~0.90 | ~0.90 | High |
| **LSTM** | 86.8% ğŸ¥‰ | 0.870 | 0.868 | 0.867 | 87.5% |

### âš¡ Efficiency Comparison

| Metric | BERT | BiLSTM+BERT | LSTM |
|--------|------|-------------|------|
| **Training Time** | ~15-20 min | ~12-15 min | **~10 min** âœ… |
| **Model Size** | 110M params | ~60M params | **1-2M params** âœ… |
| **Memory (GPU)** | 6-8 GB | 4-6 GB | **2-3 GB** âœ… |
| **Inference Speed** | Slow | Medium | **Fast** âœ… |

### ğŸ¯ Class-wise Performance

#### **Negative Class** (Most Successful)
- BERT: **98%** ğŸ†
- BiLSTM+BERT: ~95%
- LSTM: 93%

#### **Positive Class**
- BERT: **94%** ğŸ†
- BiLSTM+BERT: ~92%
- LSTM: 90%

#### **Neutral Class** âš ï¸ (Weak in All Models)
- BERT: **84%** ğŸ†
- BiLSTM+BERT: ~82%
- LSTM: 75%

### ğŸ” Key Findings

1. **BERT Highest Accuracy**: Best performance at 92.6%
2. **LSTM Most Efficient**: Least resources, fastest training
3. **BiLSTM+BERT Good Balance**: Performance-efficiency balance
4. **Neutral Class Challenging**: Improvement needed in all models
5. **Gemini High Consistency**: 87-92% agreement range
6. **GAN Successful Generation**: 1000+ unique Turkish sentences

### ğŸ¤” Which Model to Choose?

| Scenario | Recommended Model | Why? |
|----------|-------------------|------|
| **Maximum Accuracy** | BERT | Highest accuracy (92.6%) |
| **Mobile/Embedded** | LSTM | Lightest model (1-2M params) |
| **Balanced Solution** | BiLSTM+BERT | Good performance + acceptable resources |
| **Real-time** | LSTM | Fastest inference time |
| **Data Generation** | GAN + Gemini | Unique and quality synthetic data |

---

## ğŸ“ Project Structure

```
synthetic-data-generation-nlp/
â”‚
â”œâ”€â”€ README.md                                      # Main documentation (this file)
â”œâ”€â”€ LICENSE.md                                     # License information
â”‚
â”œâ”€â”€ bert-sentiment-analysis/                       # BERT Sentiment Analysis
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti-5k.xlsx
â”‚   â”œâ”€â”€ bert_vs_gemini_sonuc_1k.xlsx
â”‚   â””â”€â”€ etiketsiz-test-gemini-etiketlenmis-1k.xlsx
â”‚
â”œâ”€â”€ bilstm-bert-hybrid/                            # BiLSTM + BERT Hybrid
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti.xlsx
â”‚   â”œâ”€â”€ etiketsiz-test-gemini-etiketlenmis.xlsx
â”‚   â””â”€â”€ bert_vs_gemini_sonuc.xlsx
â”‚
â”œâ”€â”€ lstm-sentiment/                                # LSTM Sentiment Analysis
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ egitim-veriseti-5k.xlsx
â”‚   â”œâ”€â”€ lstm_vs_gemini_sonuc_1k.xlsx
â”‚   â””â”€â”€ etiketsiz-test-gemini-etiketlenmis-1k.xlsx
â”‚
â”œâ”€â”€ gan-text-generation/                           # GAN Text Generation
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ sentences.txt                              # 5000 sentences (training)
â”‚   â”œâ”€â”€ wiki.tr.txt                                # Full dataset
â”‚   â”œâ”€â”€ uretilen_cumleler.csv
â”‚   â””â”€â”€ training_history.png
â”‚
â”œâ”€â”€ gemini-dataset-generation/                     # Gemini Data Generation
â”‚   â”œâ”€â”€ main10.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ main10.pdf
â”‚   â””â”€â”€ elektrikli_araba_1000_batch.xlsx
â”‚
â””â”€â”€ LiteratÃ¼rdeki Sentetik Veri Ãœretimi Ä°le Ä°lgili Makaleler/
    â”‚                                              # ğŸ“š Literature Review
    â”œâ”€â”€ metin/                                     # Text-based synthetic data
    â”‚   â”œâ”€â”€ Genel(arxiv.org vb.)/                 # General studies
    â”‚   â””â”€â”€ ScienceDirect & IEEE Xplore/           # Academic databases
    â”‚
    â”œâ”€â”€ gÃ¶rÃ¼ntÃ¼/                                   # Image-based synthetic data
    â”‚
    â””â”€â”€ ses/                                       # Audio-based synthetic data
```

---

## ğŸ› ï¸ Installation

### System Requirements

**Hardware:**
- GPU: NVIDIA GPU (recommended: Tesla T4 or higher)
- RAM: Minimum 8GB (recommended: 16GB+)
- Storage: ~5GB (for all models)

**Software:**
- Python 3.8+
- CUDA 11.x (for GPU)
- pip or conda

### Core Libraries

```bash
# Common for all projects
pip install pandas numpy openpyxl scikit-learn

# For BERT projects
pip install torch transformers

# For LSTM projects
pip install tensorflow

# For GAN project
pip install torch sentence-transformers

# For Gemini project
pip install google-generativeai
```

### Project-specific Installation

Detailed installation instructions are available in the README.md file in each subfolder.

---

## ğŸš€ Usage

### Quick Start

1. **Clone the Repository**
```bash
git clone https://github.com/username/synthetic-data-generation.git
cd synthetic-data-generation
```

2. **Navigate to Your Interested Project**
```bash
cd bert-sentiment-analysis/  # or lstm-sentiment, gan-text-generation, etc.
```

3. **Follow README Instructions**
Each folder's README.md contains project-specific installation and execution steps.

### Using Kaggle/Colab

All projects perform best in **GPU-enabled** environments:

1. Open Kaggle Notebook or Google Colab
2. Activate GPU T4 x2 accelerator
3. Upload relevant datasets
4. Run `main.py` code

---

## ğŸ“ˆ Performance Metrics

### Accuracy Comparison

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 92.6% BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      91.0% BiLSTM+BERT (average)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         86.8% LSTM
```

### Model Size Comparison

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 110M BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               60M BiLSTM+BERT
â–ˆ                                                            1.5M LSTM
```

### Training Time (GPU T4 x2)

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20 min BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      15 min BiLSTM+BERT
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           10 min LSTM
```

---

## ğŸ“ Lessons Learned

### âœ… Successes

1. **BERT Very Powerful**: Excellent results in Turkish with transfer learning
2. **LSTM Still Valuable**: Fast and effective for resource-constrained scenarios
3. **Hybrid Approach Good**: BiLSTM+BERT provides balance point
4. **GAN Works**: Applicable for Turkish text generation
5. **Gemini Reliable**: Consistent for labeling and comparison

### âš ï¸ Challenges

1. **Neutral Class Difficult**: Lowest performance in all models
2. **Positive-Neutral Confusion**: Most difference between models and Gemini here
3. **GAN Training Sensitive**: Hyperparameter tuning critical
4. **Resource Intensive**: BERT models require large GPU memory
5. **Turkish Data Scarcity**: Finding quality labeled data is challenging

### ğŸ”® Future Improvements

- [ ] Special model training for neutral class
- [ ] Larger datasets (10K+ sentences)
- [ ] New architectures like Transformer-XL
- [ ] Multi-task learning approaches
- [ ] Ensemble modeling (BERT + LSTM)
- [ ] Fine-tuned Turkish GPT models

---

## ğŸ“š Literature Review

This project includes a comprehensive literature review on synthetic data generation. In the **LiteratÃ¼rdeki Sentetik Veri Ãœretimi Ä°le Ä°lgili Makaleler** folder, academic studies for different data types are categorized:

### ğŸ“ Text-based Synthetic Data
- **General**: General studies compiled from various sources
- **ScienceDirect & IEEE Xplore**: Selected articles from academic databases
- Studies on GAN, LSTM, BERT, and transformer-based text generation
- Turkish and multilingual synthetic data generation approaches

### ğŸ–¼ï¸ Image-based Synthetic Data
- GAN, VAE, and Diffusion models for image generation
- Synthetic image quality evaluation metrics
- Data augmentation for computer vision applications

### ğŸ”Š Audio-based Synthetic Data
- TTS (Text-to-Speech) systems
- Data augmentation with speech synthesis
- Synthetic data for speech recognition systems

> **Note**: This folder consists of resources forming the theoretical foundation of the project and serves as a reference for researchers.

---

## ğŸ“š References and Resources

### Models

- **BERT**: [dbmdz/bert-base-turkish-cased](https://huggingface.co/dbmdz/bert-base-turkish-cased)
- **Turkish GPT-2**: [ytu-ce-cosmos/turkish-gpt2](https://huggingface.co/ytu-ce-cosmos/turkish-gpt2)
- **Gemini AI**: [Google DeepMind](https://deepmind.google/technologies/gemini/)

### Datasets

- **Turkish Wikipedia**: [Turkish Sentences Dataset](https://www.kaggle.com/datasets/mahdinamidamirchi/turkish-sentences-dataset)

### Libraries

- **Transformers**: [Hugging Face](https://huggingface.co/docs/transformers/)
- **TensorFlow**: [tensorflow.org](https://www.tensorflow.org/)
- **PyTorch**: [pytorch.org](https://pytorch.org/)

---

## ğŸ¤ Contributing

This project is currently **closed source** and does not accept contributions. However:

- ğŸ’¡ You can share your suggestions and feedback
- â­ If you like the project, you can give it a star

---

## ğŸ“§ Contact

For questions about the project:

- **Developer**: Mustafa AtaklÄ±
- **GitHub**: [github.com/mustafaatakli](https://github.com/mustafaatakli)
- **Email**: [atakliim20@gmail.com](mailto:atakliim20@gmail.com)

---

## ğŸ“„ License

```
All rights reserved Â© 2025 Mustafa AtaklÄ±.

Unauthorized use, copying, or distribution is strictly prohibited.
For detailed information, please refer to the LICENSE.md file.
```

---

## â­ Don't Forget to Star!

If you found this project useful, you can support by giving a â­ on GitHub!

### ğŸ† Project Statistics

- **Total Models**: 4 different approaches
- **Total Data**: 10K+ labeled sentences
- **Total Code**: 2000+ lines of Python
- **Development Time**: 3+ months
- **GPU Hours**: 100+ hours
- **Literature**: 3 categories (Text, Image, Audio)
- **Academic Sources**: ScienceDirect & IEEE Xplore

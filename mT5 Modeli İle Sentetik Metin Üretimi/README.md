# mT5 Modeli Ä°le Sentetik Metin Ãœretimi

Bu proje, Google'Ä±n mT5 (multilingual T5) ve TÃ¼rkÃ§e'ye Ã¶zelleÅŸtirilmiÅŸ T5 modellerini kullanarak teknoloji haber baÅŸlÄ±klarÄ±ndan sentetik metin Ã¼retimi gerÃ§ekleÅŸtirmektedir. Text-to-Text Transfer Transformer (T5) mimarisi ile paraphrase ve rewrite gÃ¶revleri Ã¼zerinden Ã¼retim yapÄ±lmaktadÄ±r.

## Proje AÃ§Ä±klamasÄ±

Proje, 100 adet gerÃ§ek teknoloji haber baÅŸlÄ±ÄŸÄ±ndan yola Ã§Ä±karak, T5 modelleri ile 3000 adet sentetik haber baÅŸlÄ±ÄŸÄ± Ã¼retmektedir. Ä°ki farklÄ± model kullanÄ±lmÄ±ÅŸtÄ±r:
1. **google/mt5-base**: Genel amaÃ§lÄ± Ã§ok dilli T5
2. **Turkish-NLP/t5-efficient-base-turkish**: TÃ¼rkÃ§e'ye Ã¶zel optimize edilmiÅŸ T5

Ãœretilen veriler, diÄŸer yÃ¶ntemlerle (BERT, GPT-2, Gemini, LSTM) karÅŸÄ±laÅŸtÄ±rÄ±labilir standart metriklerle deÄŸerlendirilmektedir.

## Ana Ã–zellikler

### 1. T5 (Text-to-Text Transfer Transformer) Modeli

#### Model Mimarisi
T5, tÃ¼m NLP gÃ¶revlerini text-to-text formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:
```
Input:  "paraphrase: Google ÅŸov yapacak Google I/O 2025 canlÄ± yayÄ±nÄ±"
Output: "Google I/O 2025 etkinliÄŸi canlÄ± yayÄ±nlanacak"

Input:  "rewrite: 12 taksitle alÄ±nabilecek en iyi akÄ±llÄ± telefonlar"
Output: "Taksitle satÄ±n alÄ±nabilecek en iyi telefonlar"
```

#### KullanÄ±lan Modeller

**1. google/mt5-base**
- **Dil**: 101 dil (Ã§ok dilli)
- **Parametre**: ~580 milyon
- **EÄŸitim**: mC4 dataset (101 dil)
- **Avantaj**: Genel amaÃ§lÄ±, Ã§ok dilli
- **Dezavantaj**: TÃ¼rkÃ§e'ye Ã¶zel deÄŸil

**2. Turkish-NLP/t5-efficient-base-turkish**
- **Dil**: TÃ¼rkÃ§e odaklÄ±
- **Parametre**: ~220 milyon (efficient)
- **EÄŸitim**: TÃ¼rkÃ§e corpus
- **Avantaj**: TÃ¼rkÃ§e'ye optimize, daha kÃ¼Ã§Ã¼k
- **Dezavantaj**: Sadece TÃ¼rkÃ§e

### 2. Ã‡oklu Prompt Stratejileri

T5 modeli farklÄ± gÃ¶rev tanÄ±mlarÄ± ile kullanÄ±lÄ±r:

```python
strategies = [
    f"paraphrase: {prompt_text}",      # Parafraz Ã¼retimi
    f"generate similar: {prompt_text}", # Benzer cÃ¼mle Ã¼retimi
    f"rewrite: {prompt_text}",         # Yeniden yazma
    f"{prompt_text}",                  # Prefix olmadan (serbest)
]

# Her Ã¼retimde rastgele bir strateji seÃ§ilir
input_text = random.choice(strategies)
```

### 3. Generation Parametreleri

#### Temperature Sampling
```python
temperature = random.uniform(0.7, 1.1)  # KontrollÃ¼ Ã§eÅŸitlilik
# Daha dÃ¼ÅŸÃ¼k â†’ Daha anlamlÄ± ve tutarlÄ±
# Daha yÃ¼ksek â†’ Daha Ã§eÅŸitli ama riskli
```

#### Top-K Sampling
```python
top_k = random.randint(30, 60)  # SÄ±nÄ±rlÄ± kelime havuzu
# DÃ¼ÅŸÃ¼k kaliteli kelimeleri filtreler
```

#### Top-P (Nucleus) Sampling
```python
top_p = random.uniform(0.85, 0.95)  # KÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸi
# Daha konsantre seÃ§im
```

#### Tekrar Ã–nleme
```python
no_repeat_ngram_size = 3        # 3-gram tekrarlarÄ±nÄ± engelle
repetition_penalty = 1.2        # Tekrar edilen kelimelere ceza
early_stopping = True           # EOS token'da dur
```

### 4. Agresif Temizleme Pipeline'Ä±

T5 modelleri bazen istenmeyen Ã§Ä±ktÄ±lar Ã¼retebilir. 15 katmanlÄ± temizleme sÃ¼reci:

#### A. Temel Temizleme
```python
# 1. Sadece ilk satÄ±r
generated = generated.split('\n')[0].strip()

# 2. T5 Ã¶zel token'larÄ±nÄ± kaldÄ±r
generated = re.sub(r'<[^>]+>', '', generated)  # <extra_id_0>
generated = re.sub(r'\[UNK\]', '', generated)
```

#### B. Dil Filtreleme
```python
# 3. Kiril alfabesi (RusÃ§a) kontrolÃ¼
if re.search(r'[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]', generated):
    continue

# 4. Yunanca kontrolÃ¼
if re.search(r'[Î±-Ï‰Î‘-Î©]', generated):
    continue

# 5. Ã‡ince kontrolÃ¼
if re.search(r'[\u4e00-\u9fff]', generated):
    continue
```

#### C. Metadata Temizleme
```python
# 6. Ã–zel ayÄ±rÄ±cÄ±lardan sonrasÄ±nÄ± kaldÄ±r
generated = re.split(r'\s+[-â€“â€”Â»Â«â€¢|:]\s+', generated)[0].strip()

# 7. Link/site isimleri
generated = re.sub(r'https?://\S+', '', generated)
generated = re.sub(r'\w+\.com', '', generated)

# 8. Tarih/saat formatlarÄ±
generated = re.sub(r'\d{1,2}\.\d{1,2}\.\d{4}', '', generated)

# 9. Parantez iÃ§i bilgiler
generated = re.sub(r'\([^)]*\)', '', generated)

# 10. Emoji ve Ã¶zel semboller
generated = re.sub(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿]', '', generated)
```

#### D. TÃ¼rkÃ§e Karakter KontrolÃ¼
```python
# 11. En az %70 TÃ¼rkÃ§e karakter olmalÄ±
turkce_karakterler = len(re.findall(r'[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ]', generated))
toplam_karakterler = len(re.findall(r'\S', generated))

turkce_orani = turkce_karakterler / toplam_karakterler
if turkce_orani < 0.7:
    continue
```

#### E. Kalite Kontrolleri
```python
# 12. Kelime sayÄ±sÄ±: 5-20 kelime
if not (5 <= word_count <= 20 and len(generated) >= 15):
    continue

# 13. Her kelime en az 2 harf
if not all(len(word) >= 2 for word in words):
    continue

# 14. AynÄ± kelime 3+ kez tekrarlanamaz
word_counts = Counter(words)
if any(count >= 3 for count in word_counts.values()):
    continue

# 15. ArdÄ±ÅŸÄ±k aynÄ± kelime kontrolÃ¼
for i in range(len(words) - 1):
    if words[i].lower() == words[i+1].lower():
        continue  # "lar lar lar" gibi
```

### 5. Batch Generation

```python
num_return_sequences = 10  # Her seferde 10 varyant
max_new_tokens = 50        # Maksimum 50 yeni token

# Progress bar ile takip
pbar = tqdm(total=3000, desc="T5 Uretim")
```

### 6. 5 Temel Metrik Analizi

DiÄŸer yÃ¶ntemlerle standart karÅŸÄ±laÅŸtÄ±rma:

#### [1] Tekil Oran (Uniqueness)
- Her cÃ¼mlenin benzersizliÄŸi
- Ä°deal: â‰¥ %95

#### [2] BERTScore F1 (Anlamsal Benzerlik)
- BERT embeddings ile anlamsal benzerlik
- Ä°deal: â‰¥ 0.70

#### [3] Kelime Kapsama (Vocabulary Coverage)
- Orijinal kelimelerin korunma oranÄ±
- Ä°deal: â‰¥ %80

#### [4] Benzerlik Skoru (TF-IDF Cosine Similarity)
- Kelime tabanlÄ± benzerlik
- Ä°deal: 0.50 - 0.75

#### [5] Perplexity Skoru (Anlamsal DoÄŸallÄ±k)
- BERT MLM ile cÃ¼mle doÄŸallÄ±ÄŸÄ±
- Ä°deal: â‰¤ 100

## Dosya YapÄ±sÄ±

```
mT5 Modeli Ä°le Sentetik Metin Ãœretimi/
â”‚
â”œâ”€â”€ t5_sentetik_uretim.py                            # google/mt5-base modeli
â”œâ”€â”€ t5_turkish_sentetik_uretim.py                    # Turkish-NLP modeli
â”œâ”€â”€ tekonoloji-haber-baslÄ±klarÄ±.csv                  # Orijinal veri (100 baÅŸlÄ±k)
â”œâ”€â”€ t5_sentetik_teknoloji_haberleri_3000.csv         # mt5-base Ã§Ä±ktÄ±sÄ±
â”œâ”€â”€ t5_turkish_sentetik_teknoloji_haberleri_3000.csv # Turkish-NLP Ã§Ä±ktÄ±sÄ±
â””â”€â”€ t5-base-duz-model.txt                            # mt5-base log dosyasÄ±
```

## Gereksinimler

```
torch
transformers
pandas
numpy
scikit-learn
tqdm
bert-score
```

## Kurulum

```bash
pip install torch transformers pandas numpy scikit-learn tqdm bert-score
```

## KullanÄ±m

### Google mT5-base Modeli
```bash
python t5_sentetik_uretim.py
```

### Turkish-NLP T5 Modeli (Ã–nerilen)
```bash
python t5_turkish_sentetik_uretim.py
```

## Ã‡alÄ±ÅŸma AkÄ±ÅŸÄ±

1. **GPU KontrolÃ¼**: CUDA kullanÄ±labilirliÄŸi kontrol edilir
2. **Model YÃ¼kleme**:
   - mt5-base: ~580M parametre (~2.3 GB)
   - Turkish-NLP: ~220M parametre (~900 MB)
3. **Veri YÃ¼kleme**: 100 orijinal cÃ¼mle okunur
4. **Sentetik Ãœretim** (1.5-2 saat):
   - Rastgele orijinal cÃ¼mle seÃ§imi
   - Rastgele prompt stratejisi
   - Batch generation (10 cÃ¼mle/batch)
   - 15 katmanlÄ± temizleme
   - Tekil kontrolÃ¼
   - Progress bar ile ilerleme
5. **CSV KayÄ±t**: SonuÃ§lar kaydedilir
6. **BERT YÃ¼kleme**: Metrik hesaplamalarÄ± iÃ§in
7. **5 Temel Metrik Analizi**: KapsamlÄ± deÄŸerlendirme

## Teknik Detaylar

### T5 Encoder-Decoder Mimarisi

```
Input Sequence:
  "paraphrase: Google ÅŸov yapacak Google I/O 2025 canlÄ± yayÄ±nÄ±"

Encoder:
  â†’ Self-attention layers
  â†’ Contextual representations

Decoder:
  â†’ Cross-attention to encoder
  â†’ Self-attention layers
  â†’ Autoregressive generation

Output Sequence:
  "Google I/O 2025 etkinliÄŸi canlÄ± yayÄ±nlanacak"
```

### Generation Process

```python
# 1. Tokenize input
inputs = tokenizer(input_text, return_tensors='pt').to(device)

# 2. Generate with sampling
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    num_return_sequences=10,
    temperature=0.9,
    top_k=50,
    top_p=0.90,
    do_sample=True,
    no_repeat_ngram_size=3,
    repetition_penalty=1.2
)

# 3. Decode outputs
for output in outputs:
    text = tokenizer.decode(output, skip_special_tokens=True)
    # Temizleme ve filtreleme...
```

## Performans

### google/mt5-base

| Metrik | DeÄŸer |
|--------|-------|
| Ãœretim SÃ¼resi | ~1.5-2 saat |
| Model Boyutu | ~2.3 GB |
| Parametre SayÄ±sÄ± | ~580 milyon |
| GPU Memory | ~4-5 GB |
| Tekil Oran | %100 |
| BERTScore F1 | 0.46 (DÃ¼ÅŸÃ¼k) |
| Kelime Kapsama | %50.81 (DÃ¼ÅŸÃ¼k) |

### Turkish-NLP/t5-efficient-base-turkish

| Metrik | DeÄŸer |
|--------|-------|
| Ãœretim SÃ¼resi | ~1-1.5 saat |
| Model Boyutu | ~900 MB |
| Parametre SayÄ±sÄ± | ~220 milyon |
| GPU Memory | ~2-3 GB |
| Performans | mt5-base'den daha iyi (TÃ¼rkÃ§e iÃ§in) |

## Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

### mT5 vs GPT-2 vs BERT vs Gemini vs LSTM

| Ã–zellik | mT5 | GPT-2 | BERT MLM | Gemini | LSTM |
|---------|-----|-------|----------|--------|------|
| Mimari | Encoder-Decoder | Decoder-only | Encoder-only | LLM | RNN |
| Ãœretim YÃ¶ntemi | Seq2Seq | Causal LM | Masked LM | LLM API | Char-level |
| DoÄŸallÄ±k | Orta | YÃ¼ksek | Orta-YÃ¼ksek | Ã‡ok YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| Ã‡eÅŸitlilik | YÃ¼ksek | Ã‡ok YÃ¼ksek | Orta | YÃ¼ksek | Orta |
| Anlamsal TutarlÄ±lÄ±k | Orta | Orta-YÃ¼ksek | Orta | YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| TÃ¼rkÃ§e Kalitesi | Orta (mt5-base) | Orta | YÃ¼ksek | YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| TÃ¼rkÃ§e Kalitesi | YÃ¼ksek (Turkish-NLP) | - | - | - | - |
| HÄ±z | YavaÅŸ | HÄ±zlÄ± | HÄ±zlÄ± | Orta | Orta |
| Model Boyutu | ~900MB - 2.3GB | ~500MB | ~500MB | - | ~15MB |
| Temizleme Ä°htiyacÄ± | YÃ¼ksek | Orta | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | Orta |

### Avantajlar

âœ… **Encoder-Decoder**: Hem anlama hem Ã¼retme kapasitesi
âœ… **Ã‡ok Dilli**: 101 dil desteÄŸi (mt5-base)
âœ… **Task Flexibility**: FarklÄ± gÃ¶revler iÃ§in kullanÄ±labilir
âœ… **TÃ¼rkÃ§e Ã–zel**: Turkish-NLP modeli optimize edilmiÅŸ
âœ… **YÃ¼ksek Ã‡eÅŸitlilik**: FarklÄ± prompt stratejileri
âœ… **Pre-trained**: BÃ¼yÃ¼k veri ile eÄŸitilmiÅŸ

### Dezavantajlar

âš ï¸ **YavaÅŸ Ãœretim**: 1.5-2 saat (3000 cÃ¼mle iÃ§in)
âš ï¸ **BÃ¼yÃ¼k Model**: 2.3 GB (mt5-base)
âš ï¸ **YÃ¼ksek Memory**: 4-5 GB GPU memory
âš ï¸ **Temizleme Gereksinimi**: 15 katmanlÄ± temizleme
âš ï¸ **DÃ¼ÅŸÃ¼k Kalite**: mt5-base TÃ¼rkÃ§e iÃ§in optimize deÄŸil
âš ï¸ **Dil KarÄ±ÅŸmasÄ±**: Bazen diÄŸer dillere kayabilir

## SÄ±nÄ±rlamalar

1. **mt5-base TÃ¼rkÃ§e Sorunu**: Ã‡ok dilli model TÃ¼rkÃ§e'de dÃ¼ÅŸÃ¼k performans
2. **Dil KirliliÄŸi**: Bazen RusÃ§a, Yunanca karakterler Ã¼retir
3. **AnlamsÄ±z Ã‡Ä±ktÄ±lar**: Ã–zel token'lar (<extra_id_X>)
4. **YavaÅŸ SÃ¼reÃ§**: Batch generation ile bile yavaÅŸ
5. **YÃ¼ksek Memory**: BÃ¼yÃ¼k model boyutu
6. **Paraphrase SÄ±nÄ±rlamasÄ±**: Bazen orijinale Ã§ok benzer

## Ä°yileÅŸtirme Ã–nerileri

### Model SeÃ§imi
```python
# âœ“ Ã–NERÄ°LEN: TÃ¼rkÃ§e'ye Ã¶zel model
MODEL_NAME = 'Turkish-NLP/t5-efficient-base-turkish'

# âœ— Ã–NERÄ°LMEZ: Genel amaÃ§lÄ± (TÃ¼rkÃ§e'de zayÄ±f)
MODEL_NAME = 'google/mt5-base'
```

### Generation Parametreleri
```python
# Daha anlamlÄ± Ã¼retim iÃ§in
temperature = random.uniform(0.5, 0.9)  # (varsayÄ±lan: 0.7-1.1)
top_k = random.randint(20, 40)         # (varsayÄ±lan: 30-60)
top_p = random.uniform(0.80, 0.90)     # (varsayÄ±lan: 0.85-0.95)

# Daha Ã§eÅŸitli Ã¼retim iÃ§in
temperature = random.uniform(0.9, 1.3)
top_k = random.randint(50, 80)
top_p = random.uniform(0.90, 0.98)
```

### Batch Size ArtÄ±rma
```python
num_return_sequences = 15  # (varsayÄ±lan: 10)
# Daha hÄ±zlÄ± Ã¼retim ama daha fazla memory
```

### Prompt Optimizasyonu
```python
# Sadece etkili prompt'larÄ± kullan
strategies = [
    f"paraphrase: {prompt_text}",
    f"rewrite: {prompt_text}",
]
# "generate similar" ve boÅŸ prefix'i Ã§Ä±kar
```

## Sorun Giderme

### DÃ¼ÅŸÃ¼k BERTScore F1 (mt5-base)
```
BERTScore F1: 0.46 (DÃ¼ÅŸÃ¼k)
```
**Ã‡Ã¶zÃ¼m**:
```python
# Turkish-NLP modelini kullan
MODEL_NAME = 'Turkish-NLP/t5-efficient-base-turkish'
```

### Dil KirliliÄŸi (RusÃ§a, Yunanca)
```
Ãœretilen: "Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Samsung"
```
**Ã‡Ã¶zÃ¼m**: Zaten kod iÃ§inde Kiril/Yunan alfabesi filtreleme var (Ã§alÄ±ÅŸÄ±yor)

### GPU Memory HatasÄ±
```
RuntimeError: CUDA out of memory
```
**Ã‡Ã¶zÃ¼m**:
```python
# Batch size'Ä± azalt
num_return_sequences = 5  # (varsayÄ±lan: 10)

# Veya daha kÃ¼Ã§Ã¼k model kullan
MODEL_NAME = 'Turkish-NLP/t5-efficient-base-turkish'  # 220M < 580M
```

### Ã‡ok YavaÅŸ Ãœretim
```
T5 Uretim: 2%|â–ˆ | 60/3000 [10:00<8:20:00]
```
**Ã‡Ã¶zÃ¼m**:
```python
# Batch size artÄ±r (memory yeterse)
num_return_sequences = 15

# Veya hedef sayÄ±yÄ± azalt
TARGET_SENTENCES = 1000  # (varsayÄ±lan: 3000)
```

### AnlamsÄ±z Ã–zel Token'lar
```
Ãœretilen: "teknoloji <extra_id_0> haberleri <extra_id_1>"
```
**Ã‡Ã¶zÃ¼m**: Zaten kod iÃ§inde temizleme var:
```python
generated = re.sub(r'<[^>]+>', '', generated)
```

## GeliÅŸmiÅŸ Teknikler

### Fine-tuning T5 (Ä°leri Seviye)

```python
# Kendi domain'inize fine-tune edin
from transformers import Trainer, TrainingArguments

# Dataset hazÄ±rlayÄ±n (paraphrase Ã§iftleri)
train_dataset = [
    ("paraphrase: Orijinal cÃ¼mle 1", "Parafraz 1"),
    ("paraphrase: Orijinal cÃ¼mle 2", "Parafraz 2"),
    ...
]

# Fine-tune
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir='./t5-teknoloji-finetuned',
        num_train_epochs=3,
        per_device_train_batch_size=8
    ),
    train_dataset=train_dataset
)

trainer.train()
```

### Constrained Decoding

```python
# Sadece belirli kelimeleri kullan
from transformers import LogitsProcessor

class KeywordLogitsProcessor(LogitsProcessor):
    def __call__(self, input_ids, scores):
        # Teknoloji keyword'lerini tercih et
        return scores

model.generate(..., logits_processor=[KeywordLogitsProcessor()])
```

### Beam Search (Greedy Search Yerine)

```python
outputs = model.generate(
    **inputs,
    num_beams=5,           # Beam search (do_sample=False olmalÄ±)
    num_return_sequences=5,
    early_stopping=True
)
# Daha tutarlÄ± ama daha az Ã§eÅŸitli
```

## T5 Teorisi

### Text-to-Text Framework

T5, tÃ¼m NLP gÃ¶revlerini text-to-text formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:

```
Translation:       "translate English to German: Hello" â†’ "Hallo"
Summarization:     "summarize: Long text..." â†’ "Short summary"
Question Answering: "question: What is X?" â†’ "Answer"
Paraphrase:        "paraphrase: Original" â†’ "Paraphrased"
```

### Encoder-Decoder Advantage

```
BERT (Encoder-only):   âœ“ Understanding  âœ— Generation
GPT-2 (Decoder-only):  âœ— Understanding  âœ“ Generation
T5 (Encoder-Decoder):  âœ“ Understanding  âœ“ Generation
```

## Model KarÅŸÄ±laÅŸtÄ±rmasÄ± Ã–zeti

### Ne Zaman T5 KullanÄ±lmalÄ±?

âœ… **Paraphrase/Rewrite GÃ¶revleri**: T5 bu gÃ¶revler iÃ§in tasarlanmÄ±ÅŸ
âœ… **Ã‡ok Dilli Uygulama**: mt5-base 101 dil destekler
âœ… **Task Flexibility**: FarklÄ± gÃ¶revler iÃ§in aynÄ± model
âœ… **TÃ¼rkÃ§e Ã–zel Model Var**: Turkish-NLP optimize edilmiÅŸ

âŒ **HÄ±z Ã–ncelikli**: GPT-2 veya BERT daha hÄ±zlÄ±
âŒ **DÃ¼ÅŸÃ¼k Memory**: LSTM daha hafif
âŒ **En YÃ¼ksek Kalite**: Gemini API daha iyi
âŒ **TÃ¼rkÃ§e + Genel Model**: mt5-base TÃ¼rkÃ§e'de zayÄ±f

## Referanslar

- **T5 Paper**: [Raffel et al., 2020 - Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683)
- **mT5 Paper**: [Xue et al., 2021 - mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)
- **google/mt5-base**: [Hugging Face Model](https://huggingface.co/google/mt5-base)
- **Turkish-NLP T5**: [Hugging Face Model](https://huggingface.co/Turkish-NLP/t5-efficient-base-turkish)
- **BERTScore**: [Zhang et al., 2019](https://arxiv.org/abs/1904.09675)

## Lisans

Bu projenin tÃ¼m haklarÄ± saklÄ±dÄ±r Â© 2025 Mustafa AtaklÄ±.
Ä°zinsiz kullanÄ±mÄ±, kopyalanmasÄ± veya daÄŸÄ±tÄ±mÄ± kesinlikle yasaktÄ±r.
DetaylÄ± bilgi iÃ§in lÃ¼tfen LICENSE.md dosyasÄ±na bakÄ±nÄ±z.

---

**Not**: Bu proje araÅŸtÄ±rma ve eÄŸitim amaÃ§lÄ±dÄ±r. mT5 modeli, TÃ¼rkÃ§e iÃ§in Turkish-NLP/t5-efficient-base-turkish modeli ile kullanÄ±ldÄ±ÄŸÄ±nda daha iyi sonuÃ§lar verir. google/mt5-base Ã§ok dilli olduÄŸu iÃ§in TÃ¼rkÃ§e'de dÃ¼ÅŸÃ¼k performans gÃ¶stermektedir.

# Triple Hybrid (FaktÃ¶rel + Perplexity + Gemini) + Semantik Benzerlik
# BATCH GENERATION: 100 cÃ¼mle = 1 API isteÄŸi
import google.generativeai as genai
import pandas as pd
import numpy as np
import json
import time
import re
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForCausalLM

# ============================================================================
# KONFÄ°GÃœRASYON
# ============================================================================

# API Key
GEMINI_API_KEY = 'GEMINI_API_KEY'  

# Model parametreleri
BATCH_SIZE = 100  
TARGET_SAMPLES = 1000  # Hedef cÃ¼mle
SIMILARITY_THRESHOLD = 0.90  # Semantik benzerlik eÅŸiÄŸi
QUALITY_THRESHOLD = 0.60  # Minimum kalite skoru
USE_PERPLEXITY = True  
USE_GEMINI_VALIDATION = False  

# ============================================================================
# MODELLERÄ° YÃœKLEME
# ============================================================================

print("\n" + "ELEKTRÄ°KLÄ° ARABA VERÄ° SETÄ°" * 40)
print("ELEKTRÄ°KLÄ° ARABA VERÄ° SETÄ° OLUÅTURUCU v5.0")
print("BATCH GENERATION SÄ°STEMÄ°")
print("100 CÃ¼mle = 1 API Ä°steÄŸi")
print("ELEKTRÄ°KLÄ° ARABA VERÄ° SETÄ°" * 40 + "\n")
print("Modeller yÃ¼kleniyor...")

genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel('gemini-2.5-flash')
print("Gemini 2.5 Flash hazÄ±r!")

# Sentence Transformer (Semantik benzerlik iÃ§in)
semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
print("Sentence Transformer hazÄ±r!")

# Perplexity Model (Opsiyonel ama doÄŸruluk iÃ§in Ã¶nerilir)
if USE_PERPLEXITY:
    try:
        perplexity_tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-gpt2")
        perplexity_model = AutoModelForCausalLM.from_pretrained("ytu-ce-cosmos/turkish-gpt2")
        perplexity_model.eval()
        print("Perplexity modeli hazÄ±r!")
    except Exception as e:
        print(f"Perplexity modeli yÃ¼klenemedi: {e}")
        USE_PERPLEXITY = False
else:
    print("Perplexity kullanÄ±lmÄ±yor")

print("\n" + "=" * 80 + "\n")


# ============================================================================
# KALÄ°TE SKORLAMA FONKSÄ°YONLARI
# ============================================================================

def calculate_factual_quality_score(text):
    """
    FaktÃ¶rel kalite skorunu hesaplar (Rule-based)

    Args:
        text: DeÄŸerlendirilecek cÃ¼mle

    Returns:
        score: 0-1 arasÄ± skor
        factors: DetaylÄ± faktÃ¶r bilgileri
    """
    score = 0.0
    factors = {}

    # 1. Kelime SayÄ±sÄ± (0-0.20)
    words = text.split()
    word_count = len(words)

    if word_count < 3:
        word_score = 0.0
    elif 3 <= word_count <= 6:
        word_score = 0.10
    elif 7 <= word_count <= 12:
        word_score = 0.20
    elif 13 <= word_count <= 15:
        word_score = 0.15
    else:
        word_score = 0.05

    score += word_score
    factors['word_count_score'] = word_score
    factors['word_count'] = word_count

    # 2. Dilbilgisi ve YapÄ± (0-0.30)
    grammar_score = 0.0

    # BÃ¼yÃ¼k harf ile baÅŸlama
    if text and text[0].isupper():
        grammar_score += 0.10
        factors['starts_with_capital'] = True
    else:
        factors['starts_with_capital'] = False

    # Noktalama ile bitme
    if text and text[-1] in '.!?':
        grammar_score += 0.10
        factors['ends_with_punctuation'] = True
    else:
        factors['ends_with_punctuation'] = False

    # Kelime Ã§eÅŸitliliÄŸi
    unique_words = len(set(words))
    word_diversity = unique_words / max(1, word_count)
    if word_diversity > 0.8:
        grammar_score += 0.10
        factors['high_word_diversity'] = True
    else:
        factors['high_word_diversity'] = False

    score += grammar_score
    factors['grammar_score'] = grammar_score
    factors['word_diversity'] = round(word_diversity, 2)

    # 3. Anahtar Kelimeler (0-0.30)
    keywords = {
        'temel': ['elektrikli', 'araba', 'araÃ§', 'otomobil', 'taÅŸÄ±t'],
        'markalar': ['tesla', 'bmw', 'mercedes', 'audi', 'nissan', 'renault', 'togg',
                     'volkswagen', 'hyundai', 'kia', 'ford', 'porsche'],
        'teknik': ['batarya', 'pil', 'ÅŸarj', 'menzil', 'km', 'kwh', 'motor',
                   'gÃ¼Ã§', 'tork', 'hÄ±z', 'performans'],
        'Ã§evre': ['emisyon', 'karbon', 'temiz', 'yeÅŸil', 'sÃ¼rdÃ¼rÃ¼lebilir',
                  'Ã§evre', 'Ã§evreci'],
        'ekonomi': ['fiyat', 'maliyet', 'tasarruf', 'teÅŸvik', 'ucuz', 'pahalÄ±',
                    'ekonomik', 'bÃ¼tÃ§e']
    }

    text_lower = text.lower()
    keyword_categories = 0
    keyword_total = 0

    for category, words_list in keywords.items():
        found = sum(1 for kw in words_list if kw in text_lower)
        if found > 0:
            keyword_categories += 1
            keyword_total += found

    keyword_score = min(keyword_categories * 0.10, 0.30)
    score += keyword_score
    factors['keyword_score'] = keyword_score
    factors['keyword_categories'] = keyword_categories
    factors['keyword_total'] = keyword_total

    # 4. Bilgi Ä°Ã§eriÄŸi (0-0.20)
    info_score = 0.0

    # SayÄ±sal veri varlÄ±ÄŸÄ±
    numbers = re.findall(r'\d+', text)
    if numbers:
        info_score += 0.10
        factors['has_numbers'] = True
    else:
        factors['has_numbers'] = False

    # Ã–zel isim (bÃ¼yÃ¼k harfle baÅŸlayan kelimeler)
    proper_nouns = [w for w in words if w and w[0].isupper()]
    if len(proper_nouns) > 0:
        info_score += 0.05
        factors['has_proper_nouns'] = True
    else:
        factors['has_proper_nouns'] = False

    # CÃ¼mle uzunluÄŸu (minimum bilgi iÃ§eriÄŸi)
    if word_count >= 5:
        info_score += 0.05
        factors['sufficient_length'] = True
    else:
        factors['sufficient_length'] = False

    score += info_score
    factors['info_score'] = info_score

    # Final score
    final_score = round(min(score, 1.0), 2)
    factors['total_score'] = final_score

    return final_score, factors


def calculate_perplexity_score(text):
    """
    Perplexity skorunu hesaplar (DoÄŸallÄ±k Ã¶lÃ§Ã¼sÃ¼)

    Args:
        text: DeÄŸerlendirilecek cÃ¼mle

    Returns:
        score: 0-1 arasÄ± normalize edilmiÅŸ skor
    """
    if not USE_PERPLEXITY:
        return 0.80  # VarsayÄ±lan skor

    try:
        # Tokenize
        inputs = perplexity_tokenizer(text, return_tensors="pt")

        # Perplexity hesapla
        with torch.no_grad():
            outputs = perplexity_model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            perplexity = torch.exp(loss).item()

        # Skorlama (dÃ¼ÅŸÃ¼k perplexity = yÃ¼ksek skor)
        if perplexity < 10:
            score = 1.0
        elif perplexity < 20:
            score = 0.95
        elif perplexity < 30:
            score = 0.85
        elif perplexity < 50:
            score = 0.70
        elif perplexity < 80:
            score = 0.50
        elif perplexity < 120:
            score = 0.30
        else:
            score = 0.10

        return round(score, 2)

    except Exception as e:
        print(f"Perplexity hesaplama hatasÄ±: {e}")
        return 0.70  # Hata durumunda orta skor


def calculate_dual_quality_score(text):
    """
    Dual Hybrid Skor (FaktÃ¶rel + Perplexity)
    Gemini validasyon batch'te yapÄ±ldÄ±ÄŸÄ± iÃ§in burada kullanÄ±lmÄ±yor

    Args:
        text: DeÄŸerlendirilecek cÃ¼mle

    Returns:
        final_score: 0-1 arasÄ± final skor
        details: DetaylÄ± bilgiler
    """
    # FaktÃ¶rel skor
    factual_score, factors = calculate_factual_quality_score(text)

    # Perplexity skor
    perplexity_score = calculate_perplexity_score(text)

    # AÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ toplam
    if USE_PERPLEXITY:
        final_score = round((factual_score * 0.50) + (perplexity_score * 0.50), 2)
    else:
        final_score = factual_score

    details = {
        'factual_score': factual_score,
        'perplexity_score': perplexity_score,
        'factors': factors,
        'method': 'dual_hybrid' if USE_PERPLEXITY else 'factual_only'
    }

    return final_score, details


# ============================================================================
# SEMANTÄ°K BENZERLÄ°K
# ============================================================================

def is_similar_to_existing(new_text, existing_texts, threshold=0.90):
    """
    Yeni cÃ¼mlenin mevcut cÃ¼mlelere semantik benzerliÄŸini kontrol eder

    Args:
        new_text: Kontrol edilecek cÃ¼mle
        existing_texts: Mevcut cÃ¼mleler listesi
        threshold: Benzerlik eÅŸiÄŸi (varsayÄ±lan: 0.90)

    Returns:
        is_similar: True/False
        max_similarity: En yÃ¼ksek benzerlik skoru
        most_similar_text: En benzer cÃ¼mle
    """
    if not existing_texts:
        return False, 0.0, None

    try:
        # Embeddings
        new_embedding = semantic_model.encode(new_text)
        existing_embeddings = semantic_model.encode(existing_texts)

        
        similarities = cosine_similarity([new_embedding], existing_embeddings)[0]

        # En yÃ¼ksek benzerlik
        max_similarity = float(np.max(similarities))
        max_index = int(np.argmax(similarities))
        most_similar_text = existing_texts[max_index]

        is_similar = max_similarity >= threshold

        return is_similar, max_similarity, most_similar_text

    except Exception as e:
        print(f"Benzerlik kontrolÃ¼ hatasÄ±: {e}")
        return False, 0.0, None


# ============================================================================
# BATCH GENERATION - 100 CÃœMLE TEK Ä°STEK
# ============================================================================

def generate_batch_sentences(batch_size=100, sentiment_distribution=None,
                             focus_areas=None, iteration=1):
    """
    Tek Gemini isteÄŸinde batch_size kadar cÃ¼mle Ã¼retir

    Args:
        batch_size: Ãœretilecek cÃ¼mle sayÄ±sÄ±
        sentiment_distribution: {'pozitif': X, 'negatif': Y, 'nÃ¶tr': Z}
        focus_areas: Konu listesi
        iteration: KaÃ§Ä±ncÄ± batch (Ã§eÅŸitlilik iÃ§in)

    Returns:
        List of dicts: [{'text': '...', 'sentiment': '...'}, ...]
    """

    # VarsayÄ±lan sentiment daÄŸÄ±lÄ±mÄ±
    if sentiment_distribution is None:
        sentiment_distribution = {
            'pozitif': int(batch_size * 0.4),
            'negatif': int(batch_size * 0.2),
            'nÃ¶tr': int(batch_size * 0.4)
        }

    # VarsayÄ±lan konular
    if focus_areas is None:
        focus_areas = [
            "batarya teknolojisi ve kapasitesi",
            "ÅŸarj altyapÄ±sÄ± ve sÃ¼releri",
            "elektrikli araÃ§ fiyatlarÄ± ve maliyetleri",
            "Ã§evre faydalarÄ± ve emisyonlar",
            "performans, hÄ±z ve tork Ã¶zellikleri",
            "markalar (Tesla, BMW, Mercedes, Togg, vb.)",
            "bakÄ±m ve servis maliyetleri",
            "gelecek trendleri ve teknolojiler",
            "kullanÄ±cÄ± deneyimleri ve yorumlarÄ±",
            "teknik Ã¶zellikler ve spesifikasyonlar",
            "menzil ve batarya Ã¶mrÃ¼",
            "ÅŸarj istasyonlarÄ± ve eriÅŸilebilirlik",
            "devlet teÅŸvikleri ve destekler",
            "ikinci el pazar ve deÄŸer kaybÄ±",
            "gÃ¼venlik Ã¶zellikleri ve testleri"
        ]

    # Prompt oluÅŸtur
    prompt = f"""
Elektrikli arabalar hakkÄ±nda {batch_size} adet FARKLI ve ORÄ°JÄ°NAL TÃ¼rkÃ§e cÃ¼mle Ã¼ret.

Ã–NEMLÄ° KURALLAR:
1. Her cÃ¼mle 5-15 kelime arasÄ±nda olmalÄ±
2. Her cÃ¼mle BÄ°RBÄ°RÄ°NDEN TAMAMEN FARKLI olmalÄ± (TEKRAR YASAK!)
3. Her cÃ¼mle FARKLI bir bilgi, gÃ¶rÃ¼ÅŸ veya perspektif iÃ§ermeli
4. Ã‡eÅŸitli konulardan seÃ§: {', '.join(focus_areas[:10])}

SENTIMENT DAÄILIMI (Kesinlikle uyulmalÄ±):
- POZÄ°TÄ°F: {sentiment_distribution['pozitif']} cÃ¼mle (avantajlar, olumlu yÃ¶nler, baÅŸarÄ±lar)
- NEGATÄ°F: {sentiment_distribution['negatif']} cÃ¼mle (dezavantajlar, sorunlar, eleÅŸtiriler, zorluklar)
- NÃ–TR: {sentiment_distribution['nÃ¶tr']} cÃ¼mle (objektif bilgiler, tanÄ±mlar, rakamlar, gerÃ§ekler)

Ã‡EÅÄ°TLÄ°LÄ°K Ä°Ã‡Ä°N:
- FarklÄ± cÃ¼mle yapÄ±larÄ± kullan (soru, aÃ§Ä±klama, karÅŸÄ±laÅŸtÄ±rma)
- FarklÄ± kelime hazinesi (eÅŸ anlamlÄ± kelimeler tercih et)
- FarklÄ± uzunluklarda cÃ¼mleler (5-15 kelime arasÄ± dengeli daÄŸÄ±t)
- BazÄ±larÄ±nda sayÄ±sal veriler, bazÄ±larÄ±nda gÃ¶rÃ¼ÅŸler kullan
- FarklÄ± markalar ve modeller bahset

BATCH {iteration}: Bu batch Ã¶zellikle farklÄ± ve Ã¶zgÃ¼n olmalÄ±!

JSON formatÄ±nda dÃ¶ndÃ¼r (SADECE JSON, baÅŸka aÃ§Ä±klama ekleme):
{{
  "sentences": [
    {{"text": "CÃ¼mle 1 metni", "sentiment": "pozitif"}},
    {{"text": "CÃ¼mle 2 metni", "sentiment": "negatif"}},
    {{"text": "CÃ¼mle 3 metni", "sentiment": "nÃ¶tr"}},
    ...
  ]
}}

TEKRAR ETME! Her cÃ¼mle benzersiz olmalÄ±!
"""

    try:
        response = gemini_model.generate_content(prompt)
        result_text = response.text.strip()

        result_text = result_text.replace('```json', '').replace('```', '').strip()

        result = json.loads(result_text)
        sentences = result.get('sentences', [])

        return sentences

    except json.JSONDecodeError as e:
        print(f"JSON parse hatasÄ±: {e}")
        print(f"Response: {result_text[:200]}...")
        return []

    except Exception as e:
        print(f"Batch Ã¼retim hatasÄ±: {e}")
        return []


# ============================================================================
# BATCH Ä°ÅLEME VE FÄ°LTRELEME
# ============================================================================

def process_batch(batch_sentences, existing_texts, similarity_threshold=0.90,
                  quality_threshold=0.60, batch_num=1):
    """
    Batch'teki cÃ¼mleleri filtreler ve kaliteli olanlarÄ± seÃ§er

    Args:
        batch_sentences: Gemini'den gelen cÃ¼mle listesi
        existing_texts: Mevcut cÃ¼mleler
        similarity_threshold: Semantik benzerlik eÅŸiÄŸi
        quality_threshold: Minimum kalite skoru
        batch_num: Batch numarasÄ±

    Returns:
        accepted: Kabul edilen cÃ¼mleler
        rejected: Reddedilen cÃ¼mleler
    """

    accepted = []
    rejected = []
    temp_existing = existing_texts.copy()

    print(f"\nBATCH {batch_num}: {len(batch_sentences)} cÃ¼mle iÅŸleniyor...")

    for i, item in enumerate(batch_sentences, 1):
        try:
            text = item.get('text', '').strip()
            sentiment = item.get('sentiment', 'nÃ¶tr').lower()

            # Temizleme
            text = text.replace('"', '').replace("'", '').strip()
            if text and text[-1] not in ['.', '!', '?']:
                text += '.'

            # Sentiment normalize
            if sentiment not in ['pozitif', 'negatif', 'nÃ¶tr']:
                sentiment = 'nÃ¶tr'

            # BoÅŸ kontrol
            if not text or len(text) < 10:
                rejected.append({
                    'text': text,
                    'reason': 'Ã‡ok kÄ±sa',
                    'batch': batch_num
                })
                continue

            # Kelime sayÄ±sÄ±
            word_count = len(text.split())
            if not (3 <= word_count <= 15):
                rejected.append({
                    'text': text,
                    'reason': f'Kelime sayÄ±sÄ±: {word_count}',
                    'batch': batch_num
                })
                continue

            # Kalite skoru
            quality_score, quality_details = calculate_dual_quality_score(text)

            if quality_score < quality_threshold:
                rejected.append({
                    'text': text,
                    'reason': f'DÃ¼ÅŸÃ¼k kalite: {quality_score:.2f}',
                    'score': quality_score,
                    'batch': batch_num
                })
                continue

            # Semantik benzerlik (hem mevcut hem batch iÃ§i)
            is_similar, max_sim, similar_text = is_similar_to_existing(
                text, temp_existing, threshold=similarity_threshold
            )

            if is_similar:
                rejected.append({
                    'text': text,
                    'reason': f'Benzer: {max_sim:.3f}',
                    'similar_to': similar_text[:50] + '...',
                    'batch': batch_num
                })
                continue

           
            accepted.append({
                'text': text,
                'sentiment': sentiment,
                'word_count': word_count,
                'quality_score': quality_score,
                'max_similarity': max_sim,
                'factual_score': quality_details['factual_score'],
                'perplexity_score': quality_details['perplexity_score'],
                'batch': batch_num
            })

            temp_existing.append(text)

        except Exception as e:
            print(f"CÃ¼mle {i} iÅŸleme hatasÄ±: {e}")
            continue

    print(f"Kabul: {len(accepted)} | Red: {len(rejected)}")

    return accepted, rejected


# ============================================================================
# 1000 CÃœMLE ÃœRETÄ°MÄ° (BATCH SÄ°STEMÄ°)
# ============================================================================

def create_dataset_with_batches(target_samples=1000, batch_size=100,
                                similarity_threshold=0.90, quality_threshold=0.60):
    """
    Batch generation ile dataset oluÅŸturur

    Args:
        target_samples: Hedef cÃ¼mle sayÄ±sÄ±
        batch_size: Her batch'te kaÃ§ cÃ¼mle
        similarity_threshold: Benzerlik eÅŸiÄŸi
        quality_threshold: Kalite eÅŸiÄŸi

    Returns:
        df: Pandas DataFrame
        rejected: Reddedilen cÃ¼mleler
    """

    dataset = []
    all_rejected = []
    existing_texts = []
    batch_count = 0
    total_api_requests = 0

    print("\n" + "=" * 80)
    print("BATCH SÄ°STEM Ä°LE VERÄ° SETÄ° OLUÅTURMA")
    print("=" * 80)
    print(f"Hedef: {target_samples} cÃ¼mle")
    print(f"Batch boyutu: {batch_size} cÃ¼mle/batch")
    print(f"Benzerlik eÅŸiÄŸi: {similarity_threshold}")
    print(f"Kalite eÅŸiÄŸi: {quality_threshold}")
    print(f"Perplexity: {'Aktif' if USE_PERPLEXITY else 'KapalÄ±'}")
    print("=" * 80 + "\n")

    # Sentiment hedefleri
    sentiment_targets = {
        'pozitif': int(target_samples * 0.4),
        'negatif': int(target_samples * 0.2),
        'nÃ¶tr': int(target_samples * 0.4)
    }

    sentiment_counts = {'pozitif': 0, 'negatif': 0, 'nÃ¶tr': 0}

    start_time = time.time()

    while len(dataset) < target_samples:
        batch_count += 1

        # Kalan cÃ¼mle sayÄ±sÄ±
        remaining = target_samples - len(dataset)
        current_batch_size = min(batch_size, remaining + 30)  # +30 yedek

        # Sentiment daÄŸÄ±lÄ±mÄ± (kalan iÃ§in)
        batch_sentiment_dist = {}
        for sent, target in sentiment_targets.items():
            needed = max(0, target - sentiment_counts[sent])
            ratio = needed / max(1, remaining)
            batch_sentiment_dist[sent] = max(1, int(current_batch_size * ratio))

        # ToplamÄ± normalize et
        total_dist = sum(batch_sentiment_dist.values())
        if total_dist != current_batch_size:
            diff = current_batch_size - total_dist
            batch_sentiment_dist['nÃ¶tr'] += diff

        print(f"\n{'=' * 80}")
        print(f"BATCH {batch_count}")
        print(f"{'=' * 80}")
        print(f"Hedef: {current_batch_size} cÃ¼mle")
        print(f"Sentiment: Poz:{batch_sentiment_dist['pozitif']} "
              f"Neg:{batch_sentiment_dist['negatif']} "
              f"NÃ¶tr:{batch_sentiment_dist['nÃ¶tr']}")
        print(f"Mevcut: {len(dataset)}/{target_samples}")
        print(f"API Ä°steÄŸi: {total_api_requests + 1}")

        # Batch Ã¼ret
        print(f"Ãœretiliyor...")
        batch_sentences = generate_batch_sentences(
            batch_size=current_batch_size,
            sentiment_distribution=batch_sentiment_dist,
            iteration=batch_count
        )

        total_api_requests += 1

        if not batch_sentences:
            print("Batch boÅŸ geldi, tekrar deneniyor...")
            time.sleep(2)
            continue

        print(f"{len(batch_sentences)} cÃ¼mle alÄ±ndÄ±")

        # Filtrele
        accepted, rejected = process_batch(
            batch_sentences,
            existing_texts,
            similarity_threshold=similarity_threshold,
            quality_threshold=quality_threshold,
            batch_num=batch_count
        )

        # Kabul edilenleri ekle
        for item in accepted:
            sentiment = item['sentiment']

            # Sentiment limitini kontrol et
            if sentiment_counts[sentiment] < sentiment_targets[sentiment]:
                item['id'] = len(dataset) + 1
                dataset.append(item)
                existing_texts.append(item['text'])
                sentiment_counts[sentiment] += 1
            else:
                # Sentiment limiti dolmuÅŸ, baÅŸka sentiment'e geÃ§ir
                for alt_sent in ['pozitif', 'negatif', 'nÃ¶tr']:
                    if sentiment_counts[alt_sent] < sentiment_targets[alt_sent]:
                        item['sentiment'] = alt_sent
                        item['id'] = len(dataset) + 1
                        dataset.append(item)
                        existing_texts.append(item['text'])
                        sentiment_counts[alt_sent] += 1
                        break

        all_rejected.extend(rejected)

        # Ä°statistikler
        elapsed = time.time() - start_time
        print(f"\nBATCH {batch_count} Ã–ZET:")
        print(f"   Ãœretilen: {len(batch_sentences)}")
        print(f"   Kabul: {len(accepted)}")
        print(f"   Red: {len(rejected)}")
        print(f"   Dataset: {len(dataset)}/{target_samples} (%{len(dataset) / target_samples * 100:.1f})")
        print(f"   API Ä°steÄŸi: {total_api_requests}")
        print(f"   SÃ¼re: {elapsed / 60:.1f} dakika")

        print(f"\n   Sentiment DaÄŸÄ±lÄ±mÄ±:")
        for sent in ['pozitif', 'negatif', 'nÃ¶tr']:
            count = sentiment_counts[sent]
            target = sentiment_targets[sent]
            pct = (count / target) * 100 if target > 0 else 0
            bar = 'â–ˆ' * int(pct / 5)
            print(f"   {sent:8}: {count:3}/{target:3} (%{pct:5.1f}) {bar}")

        # Hedef kontrolÃ¼
        if len(dataset) >= target_samples:
            break

        # Rate limiting
        if len(dataset) < target_samples:
            time.sleep(1)

    # Final DataFrame
    df = pd.DataFrame(dataset[:target_samples])

    elapsed_total = time.time() - start_time

    print(f"\n{'=' * 80}")
    print("VERÄ° SETÄ° OLUÅTURULDU!")
    print(f"{'=' * 80}")
    print(f"Toplam cÃ¼mle: {len(df)}")
    print(f"Toplam batch: {batch_count}")
    print(f"Toplam API isteÄŸi: {total_api_requests}")
    print(f"Toplam sÃ¼re: {elapsed_total / 60:.1f} dakika")
    print(f"Reddedilen: {len(all_rejected)}")

    print(f"\nSentiment DaÄŸÄ±lÄ±mÄ±:")
    for sent in ['pozitif', 'negatif', 'nÃ¶tr']:
        count = len(df[df['sentiment'] == sent])
        pct = (count / len(df)) * 100
        emoji = {'pozitif': 'ğŸ˜Š', 'negatif': 'ğŸ˜”', 'nÃ¶tr': 'ğŸ˜'}[sent]
        print(f"   {emoji} {sent:8}: {count:3} (%{pct:5.1f})")

    print(f"\nOrtalama Skorlar:")
    print(f"   Quality:    {df['quality_score'].mean():.3f}")
    print(f"   FaktÃ¶rel:   {df['factual_score'].mean():.3f}")
    print(f"   Perplexity: {df['perplexity_score'].mean():.3f}")
    print(f"   Similarity: {df['max_similarity'].mean():.3f}")

    print(f"\nKelime Ä°statistikleri:")
    print(f"   Ortalama:   {df['word_count'].mean():.1f} kelime")
    print(f"   Minimum:    {df['word_count'].min()} kelime")
    print(f"   Maksimum:   {df['word_count'].max()} kelime")

    print(f"{'=' * 80}\n")

    return df, all_rejected


# ============================================================================
# ANALÄ°Z VE GÃ–RSELLEÅTÄ°RME
# ============================================================================

def analyze_dataset(df, rejected):
    """Dataset analizi ve istatistikler"""

    print("\n" + "=" * 80)
    print("DETAYLI ANALÄ°Z")
    print("=" * 80)

    # Kalite daÄŸÄ±lÄ±mÄ±
    print("\nKalite Skor DaÄŸÄ±lÄ±mÄ±:")
    bins = [0.60, 0.70, 0.80, 0.90, 1.0]  
    labels = ['DÃ¼ÅŸÃ¼k (0.60-0.70)', 'Orta (0.70-0.80)',
              'Ä°yi (0.80-0.90)', 'Ã‡ok Ä°yi (0.90-1.0)']

    df['quality_category'] = pd.cut(df['quality_score'], bins=bins, labels=labels)
    print(df['quality_category'].value_counts().to_string())

    # Batch daÄŸÄ±lÄ±mÄ±
    if 'batch' in df.columns:
        print("\nBatch BaÅŸÄ±na Kabul Edilen:")
        batch_counts = df['batch'].value_counts().sort_index()
        for batch, count in batch_counts.items():
            print(f"   Batch {batch}: {count} cÃ¼mle")

    # Red sebepleri
    if rejected:
        print("\nRed Sebepleri (Ä°lk 5):")
        reasons = {}
        for item in rejected:
            reason = item.get('reason', 'Bilinmiyor')
            reasons[reason] = reasons.get(reason, 0) + 1

        for reason, count in sorted(reasons.items(), key=lambda x: -x[1])[:5]:
            print(f"   {reason}: {count}")

    print("\n" + "=" * 80)


def save_dataset(df, rejected, prefix='dataset'):
    """Dataset'i farklÄ± formatlarda kaydet"""

    print(f"\nDosyalar kaydediliyor...")

    # CSV
    csv_file = f'{prefix}_1000_batch.csv'
    df.to_csv(csv_file, index=False, encoding='utf-8')
    print(f"   {csv_file}")

    # Excel
    excel_file = f'{prefix}_1000_batch.xlsx'
    df.to_excel(excel_file, index=False)
    print(f"   {excel_file}")

    # JSON
    json_file = f'{prefix}_1000_batch.json'
    df.to_json(json_file, orient='records', force_ascii=False, indent=2)
    print(f"   {json_file}")

    # Rejected (opsiyonel)
    if rejected:
        rejected_file = f'{prefix}_rejected.json'
        with open(rejected_file, 'w', encoding='utf-8') as f:
            json.dump(rejected, f, ensure_ascii=False, indent=2)
        print(f"   {rejected_file}")

    print(f"\nTÃ¼m dosyalar kaydedildi!")


# ============================================================================
# MAIN PROGRAM
# ============================================================================

if __name__ == "__main__":

    # Parametreler
    print("\nPARAMETRELER:")
    print(f"   Hedef: {TARGET_SAMPLES} cÃ¼mle")
    print(f"   Batch boyutu: {BATCH_SIZE}")
    print(f"   Benzerlik eÅŸiÄŸi: {SIMILARITY_THRESHOLD}")
    print(f"   Kalite eÅŸiÄŸi: {QUALITY_THRESHOLD}")
    print(f"   Perplexity: {'Aktif' if USE_PERPLEXITY else 'KapalÄ±'}")

    # Onay
    print("\n" + "=" * 80)
    user_input = input("BaÅŸlatmak iÃ§in ENTER'a basÄ±n (Ã‡Ä±kmak iÃ§in 'q'): ")
    if user_input.lower() == 'q':
        print("Ä°ptal edildi.")
        exit()

    # Dataset oluÅŸtur
    df, rejected = create_dataset_with_batches(
        target_samples=TARGET_SAMPLES,
        batch_size=BATCH_SIZE,
        similarity_threshold=SIMILARITY_THRESHOLD,
        quality_threshold=QUALITY_THRESHOLD
    )

    # Analiz
    analyze_dataset(df, rejected)

    # Kaydet
    save_dataset(df, rejected, prefix='elektrikli_araba')

    # Final mesaj
    print("\n" + "ELEKTRÄ°KLÄ° ARABA VERÄ° SETÄ°" * 40)
    print("BAÅARILI! 1000 CÃœMLE ÃœRETÄ°LDÄ°!")
    print("ELEKTRÄ°KLÄ° ARABA VERÄ° SETÄ°" * 40)
    print(f"\nOrtalama kalite: {df['quality_score'].mean():.3f}")
    print(f"Unique cÃ¼mleler: %100")
    print(f"API isteÄŸi kullanÄ±mÄ±: Minimal (~10-15 istek)")
    print(f"Maliyet: ~")
    print("\nDataset hazÄ±r, kullanÄ±ma uygun!\n")